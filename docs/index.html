<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>MediaFranca: A Practice-Oriented Investigation into a Generative Pictographic System for Cognitive Accessibility</title>
  <meta name="description" content="Confirmation of Candidature presentation for the Doctor of Philosophy in Design at Auckland University of Technology (AUT).">
  <meta name="keywords" content="aac, pictograms, generative-ai, practice-oriented research, interaction design">
  <meta name="author" content="Herbert Spencer González">
  <meta name="robots" content="index, follow">
  <meta name="language" content="en-NZ">

  <!-- Dublin Core Metadata -->
  <meta name="DC.title" content="MediaFranca: A Practice-Oriented Investigation into a Generative Pictographic System for Cognitive Accessibility">
  <meta name="DC.creator" content="Herbert Spencer González">
  <meta name="DC.description" content="Confirmation of Candidature presentation for the Doctor of Philosophy in Design at Auckland University of Technology (AUT).">
  <meta name="DC.subject" content="AAC, pictograms, generative AI, design research, accessibility, interaction design">
  <meta name="DC.type" content="Presentation">
  <meta name="DC.format" content="text/html">
  <meta name="DC.identifier" content="https://herbertspencer.net/cc">
  <meta name="DC.source" content="Auckland University of Technology (AUT)">
  <meta name="DC.coverage" content="Auckland, New Zealand">
  <meta name="DC.rights" content="Creative Commons Attribution 4.0 International (CC BY 4.0)">
  <meta name="DC.language" content="en-NZ">
  <meta name="DC.date" content="2025-10-31">

  <!-- Open Graph -->
  <meta property="og:title" content="MediaFranca: A Practice-Oriented Investigation into a Generative Pictographic System for Cognitive Accessibility">
  <meta property="og:description" content="Confirmation of Candidature presentation for the Doctor of Philosophy in Design at Auckland University of Technology (AUT).">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://herbertspencer.net/cc">
  <meta property="og:locale" content="en_NZ">
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/2.0.4/p5.min.js" defer></script>
  <script defer src="/cc/p5/sketches.js"></script>
  <script type="module" src="/cc/svg-sync.js"></script>
  <script type="module" crossorigin src="/cc/assets/index-p0YdC-_2.js"></script>
  <link rel="stylesheet" crossorigin href="/cc/assets/index-Aabcoknr.css">
</head>

<body>
  <div class="reveal">
    <div class="slides">

      <!-- 1: Introduction  -->
      <section id="intro">
        <!-- 1.1 title -->
        <section data-background-color="#FFF" data-transition="concave" id="title-slide">
          <object type="image/svg+xml" data="svg/title-illustration.svg" class="illustration"
            alt="MediaFranca Illustration: a thinking bubble of pictograms"></object>

          <div class="col-special">
            <div class="title">
              <h1><span>MediaFranca:</span>A Practice-Oriented Investigation into a Generative Pictographic System for
                Cognitive Accessibility</h1>
            </div>

            <div class="credentials">
              <div class="author">
                <dt>Candidate</dt>
                <dl><strong>Herbert Spencer González</strong><br>
                  <span>PhD Student in Design</span>
                </dl>

              </div>

              <div class="tutors">
                <dt>Supervisors</dt>
                <dl>Dr. Marcos Steagall</dl>
                <dl>Dr. Ivana Nakarada-Kordic</dl>
                <dt>Advisor</dt>
                <dl>Dr. Welby Ings</dl>
              </div>
            </div>
          </div>

          <div class="footer">
            <span>This presentation lives at <a href="https://herbertspencer.net/cc">herbertspencer.net/cc</a></span>
            <img class="logo" src="images/AUT.jpg" alt="Auckland University of Technology">
          </div>
        </section>

        <!-- 1.2 Aromos  -->
        <section data-background-image="images/aromos.jpg">
          <aside class="notes">This is my home in Chile, near the Aconcagua river and Cerro la Campana. I begin here
            because design is always situated; it begins from place, from the ground where we stand and from the waters
            that sustain us.</aside>
        </section>


        <!-- 1.3 Profesor  -->
        <section data-background-image="images/travesia.jpg">
          <div class="figure-legend">
            <div class="overlay">
              <strong>Travesía 2018, Corredor Litoral</strong>.<br>First Year Design Studio + Third year Light and Color Design Studio
            </div>
          </div>
          <aside class="notes">Here you see my students during a design "Travesía" in the north of Chile. We travelled
            where the Pacific meets the Atacama desert and built, at full scale, a timber “coastal corridor”. For me,
            teaching design has always meant expanding perception, opening encounters, and making space for others to
            participate.</aside>
        </section>

        <!-- Núcleo de Investigación en Accesibilidad e Inclusión PUCV  -->
        <section data-background-image="images/nucleo-2.jpg">
          <div class="figure-legend">
            <div class="overlay">
              <strong>Research Centre for Accessibility and Inclusion</strong> + friends<br>Pontifical Catholic University of Valparaíso
            </div>
          </div>
          <aside class="notes">
            Over time, the team grew into an interdisciplinary centre, with people from linguistics, engineering,
            sociology, law and more. Together we tried to build knowledge across differences, following the idea that
            inclusive research requires inclusive teams.
          </aside>
        </section>

        <!-- 1.5 PICTOS Service  -->
        <section class="w">
          <img class="fit" src="images/pictos-service.png" alt="PICTOS Sevice Arqchitecture">
          <aside class="notes">
            So we designed supports: a grammar for pictograms, a system for plain language, and a publishing
            architecture to adapt these supports across services. The aim was not to simplify life but to return
            dignity, to enable people to complete everyday transactions without dependence... and get the value that
            these services offer.
          </aside>
        </section>

        <!-- the space between words and images -->
        <section data-background-color="#FFF" data-background-image="svg/word-visual-space.svg">
          <aside class="notes">Both groups converge here: they say cognitive accessibility depends on coherence between
            words and images. Therapists describe the fatigue of their students when facing mismatched systems.
            Designers describe the failure of icons when they drift into opacity or cliché. Their insights confirm the
            hypothesis: the corridor between word and image must remain tight, structured, and open to correction.
            The PICTOS approach consists of a highly coherent support between the word space and the image. The direct
            correspondence between instruction and pictogram promotes comprehension and reduces cognitive overload. This
            is the fundamental idea on which my research is based.</aside>
        </section>

        <!-- 1.3 research question-->
        <section>
          <p class="smallcaps">research question:</p>
          <p class="huge question">
            How can a generative pictographic system <span>be designed</span> to support
            communication for people with complex communication needs?
          </p>
          <aside class="notes">The core question I bring here is: how can a generative pictographic system be designed
            to support communication for people with complex communication needs? To approach this, I am not working
            alone in my studio. I am conducting interviews with two groups of professionals. On the one hand, those who
            teach, educate, and promote the use of AAC tools in schools, clinics, and daily practices. On the other
            hand, designers who have dedicated their craft to pictograms with great concern for accessibility,
            comprehension, synthesis, and elegance. Their voices frame the territory of this research.</aside>
        </section>

      </section><!-- 1 end -->

      <!-- 2: Why this research matters -->
      <section id="why-matters">
        <!-- 2.1 Scale of need (Aotearoa NZ) -->
        <section data-background-color="#FFF">
          <h2 class="xl">Why this research matters?</h2>
          <div class="image">
            <img src="svg/people-with-ccn.svg" alt="between 6% and 10% have complex communication needs">
            <aside class="notes">
              <div class="legend">
                <ul>
                  <li><strong>1 in 6</strong> New Zealanders (≈ 851,000 people) live with disability (2023 Census).</li>
                  <li><strong>5.9%</strong> of people aged 5+ report some difficulty communicating.</li>
                  <li>Professional estimates suggest <strong>up to 10%</strong> experience communication difficulties.
                  </li>
                </ul>
              </div>
            </aside>
          </div>
        </section>

        <!-- 2.2 What AAC is (plain) -->
        <section data-background-image="images/aac-systems.png" data-transition="fade" class="invert">

          <div style="text-align: center; width: 60%; margin: 0 auto">
            <img src="svg/aac.svg" alt="Universal Accessibility Pictogram + a speaking bubble with a check sign"
              style="width: 200px;">
            <h2 style="display: block; text-align: center;">Augmentative and Alternative Communication (AAC)</h2>
            <p><span lang=EN-NZ>AAC is an <b>Interdisciplinary field</b> and a <b>community of practice</b> that combines technologies, symbols, and interaction strategies to <b>support</b> and <b>extend</b> communication by <b>adding new channels</b>, enabling individuals with complex communication needs to <b>participate meaningfully</b> in social and communicative life.</span></p>

          </div>
          <aside class="notes">
            <ul style="color: white">
              <li>AAC refers to systems and practices that support individuals with limited or absent speech through
                multimodal means such as symbols, gestures, or technologies</li>
              <li>It views communication as a social, intentional, and context-dependent process rather than a purely
                linguistic act, emphasising meaning exchange and participation</li>
              <li>AAC strives to enable communicative autonomy and social inclusion, allowing individuals to express
                intent, share understanding, and participate fully in everyday life</li>
            </ul>
          </aside>

        </section>

        <!-- 2.3 A gap in representation -->
        <section data-transition="fade">
          <h2 class="xl">A persistent gap</h2>
          <table class="gap">
            <thead>
              <tr>
                <th><img src="svg/noun-003.svg"><br>Speech & Language Augmentation</th>
                <th><img src="svg/noun-004.svg"><br>Predictive Communication Systems</th>
                <th><img src="svg/noun-002.svg"><br>Embodied / Sensor-Based Interfaces</th>
                <th><img src="svg/noun-001.svg"><br>Pictographic & Symbolic Representation</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <ul>
                    <li class="check">voice recognition</li>
                    <li class="check">text-to-speech synthesis</li>
                    <li class="check">speech-to-text captioning</li>
                    <li class="check">AI speech repair for dysarthria</li>
                  </ul>
                </td>
                <td>
                  <ul>
                    <li class="check">predictive text</li>
                    <li class="check">next pictogram prediction</li>
                    <li class="check">context-aware phrase suggestion</li>
                    <li class="check">adaptive keyboards</li>
                  </ul>
                </td>
                <td>
                  <ul>
                    <li class="check">swipe keyboards</li>
                    <li class="check">eye tracking</li>
                    <li class="check">gesture control</li>
                    <li class="check">facial expression detection</li>
                    <li class="check">EEG-based intent sensing</li>
                  </ul>
                </td>
                <td>
                  <ul>
                    <li class="check">static pictogram sets</li>
                    <li class="fail">no dynamic generative pictographic systems</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
        </section>
      </section><!-- 2 end -->

      <!-- Separator: Part 2 - Contextual Review -->
      <section data-background-color="black">
        <h1 class="divider"><span>Part 2</span><br>Contextual Review</h1>
      </section>

      <!-- 3: Literature review: The Problem of Language -->
      <section id="problem-of-language">

        <!-- 3.1 Babel -->
        <section data-background-image="images/babel.jpg" class="invert">
          <div class="figure-legend" style="text-align: right; width: 100%;">
            <div style="width: 10em; background-color: rgba(0, 0, 0, 0.351); float: right; padding: 1ex 1em;"><strong>The Tower of
                Babel</strong>, oil on panel by Pieter Bruegel the Elder, 1563.</div>
          </div>
        </section>

        <!-- 3.2 Characteristica Universalis -->
        <!-- bye 
        <section data-background-image="images/characteristica_universalis.jpg">
          <div class="figure-legend"><strong>Characteristica Universalis</strong>, diagrammatic representation of
            elemental relations from <em>De Arte Combinatoria</em>, Gottfried Wilhelm Leibniz, 1666.</div>
        </section> -->

        <!-- 3.3 Fregge Concept Script -->
        <section data-background-image='images/begriffsschrift.png'>
          <div class="figure-legend"><Strong>Begriffsschrift</Strong>, diagrammatic notation from <em>Concept
              Script</em>, Gottlob Frege, 1879.</div>
        </section>

        <!-- Math: the truly universal language -->
        <section data-background-image='svg/math.svg' data-background-color="black">
    
        </section>

        <!-- 3.4 Wittgenstein's Rabbit -->
        <section data-background-image='images/relativity.png'>
          
          <aside class="notes">The image illustrates the shift between two aspects of perception — seeing the same
            drawing either as a duck or as a rabbit — used by Wittgenstein to show that meaning depends on the way
            something is seen or interpreted within a language game, rather than on the image itself.</aside>
        </section>

        <!-- 3.5 The ladder of Abstraction -->
        <section data-background-color="white">
          <img src="images/ladder-of-abstraction.png" alt="The ladder of abstraction" style="width: 100%;">
        </section>

        <!-- 3.6 Dual Coding Theory -->
        <section>
          <div style="text-align: center;">
            <img src="svg/dual-coding-theory.svg" alt="Dual Coding Theory" width="90%">
          </div>
        </section>

        <!-- 3.7 Universal-Local Paradox -->
        <section id="situated-language" data-transition="fade">
          <h3>Universal-local paradox</h3>

          <div class="r-stack">
            <div class= "part" height="90vh" style="background-color: white; text-align: center;">
              <img src="images/img-06.png" width="70%">
              <p class="label">Illustration from “Language, Thought, and Reality: Selected Writings of Benjamin Lee
                Whorf”, 1956</p>
            </div>
            <div class="fragment part" height="90vh" style="background-color: white; text-align: center;">
              <img src="images/img-07.png" width="70%">
              <p class="label">The <strong>"Maluma-takete"</strong> effect (later renamed “Bouba-kiki”), Köhler, 1929.
              </p>
            </div>
          </div>

          <aside class="notes">
            The image illustrates the Sapir–Whorf hypothesis, showing how English and Nootka describe the same event
            through different grammatical worldviews. In English, meaning is organised through nouns and actions (“He
            invites people to a feast”), while in Nootka it unfolds as a single verbal complex representing processes
            and relations (“boil-ed-eat-ers-go-for-he-does”).
            This contrast exemplifies the paradox of linguistic relativity: language is at once a universal human
            faculty and a local cultural system that shapes how reality is experienced. Where universality lies in the
            human capacity for symbolisation, locality appears in the grammar through which each community structures
            experience.
          </aside>
        </section>

        <!-- 3.8 NSM Semantic Primes -->
        <section data-transition="slide" data-background-color="#000">
          <h2 class="title">65 semantic primes</h2>

          <main class="nsm">
            <!-- 1. Substantives -->
            <div>
              <h2 class="block-name">Substantives</h2>
              <ul class="nsm-block">
                <!-- dificultad de uno a cinco -->
                <li class="d3">I</li>
                <li class="d3">YOU</li>
                <li class="d1">SOMEONE / PERSON</li>
                <li class="d5">SOMETHING / THING</li>
                <li class="d1">BODY</li>
                <li class="d1">PEOPLE</li>
              </ul>
            </div>

            <!-- 2. Determiners & Quantifiers -->
            <div>
              <h2 class="block-name">Determiners & Quantifiers</h2>
              <ul class="nsm-block">
                <li class="d1">THIS</li>
                <li class="d4">THE SAME</li>
                <li class="d4">OTHER / ELSE</li>
                <li class="d1">ONE</li>
                <li class="d1">TWO</li>
                <li class="d2">MANY / MUCH</li>
                <li class="d3">SOME / A FEW</li>
                <li class="d4">ALL</li>
                <li class="d5">THERE IS / EXISTS</li>
                <li class="d4">HAVE (PARTS)</li>
              </ul>
            </div>

            <!-- 3. Actions & Events -->
            <div>
              <h2 class="block-name">Actions & Events</h2>
              <ul class="nsm-block">
                <li class="d1">DO</li>
                <li class="d4">HAPPEN</li>
                <li class="d3">MOVE</li>
                <li class="d3">TOUCH</li>
              </ul>
            </div>

            <!-- 4. Mental Predicates -->
            <div>
              <h2 class="block-name">Mental Predicates</h2>
              <ul class="nsm-block">
                <li class="d3">THINK</li>
                <li class="d4">KNOW</li>
                <li class="d5">WANT</li>
                <li class="d5">FEEL</li>
                <li class="d4">SEE</li>
                <li class="d4">HEAR</li>
              </ul>
            </div>

            <!-- 5. Speech -->
            <div>
              <h2 class="block-name">Speech</h2>
              <ul class="nsm-block">
                <li class="d2">SAY</li>
                <li class="d3">WORD</li>
                <li class="d5">TRUE</li>
              </ul>
            </div>

            <!-- 6. Time & Place -->
            <div>
              <h2 class="block-name">Time & Place</h2>
              <ul class="nsm-block">
                <li class="d5">WHEN / TIME</li>
                <li class="d3">NOW</li>
                <li class="d3">BEFORE</li>
                <li class="d3">AFTER</li>
                <li class="d5">A LONG TIME</li>
                <li class="d5">A SHORT TIME</li>
                <li class="d4">WHERE / PLACE</li>
                <li class="d2">HERE</li>
                <li class="d1">ABOVE</li>
                <li class="d1">BELOW</li>
                <li class="d1">FAR</li>
                <li class="d1">NEAR</li>
                <li class="d1">SIDE</li>
                <li class="d1">INSIDE</li>
              </ul>
            </div>

            <!-- 7. Logical Concepts -->
            <div>
              <h2 class="block-name">Logical Concepts</h2>
              <ul class="nsm-block">
                <li class="d2">NOT / NO</li>
                <li class="d4">MAYBE</li>
                <li class="d4">CAN</li>
                <li class="d5">BECAUSE</li>
                <li class="d5">IF</li>
              </ul>
            </div>

            <!-- 8. Evaluators & Descriptors -->
            <div>
              <h2 class="block-name">Evaluators & Descriptors</h2>
              <ul class="nsm-block">
                <li class="d5">GOOD</li>
                <li class="d5">BAD</li>
                <li class="d3">BIG</li>
                <li class="d3">SMALL</li>
                <li class="d3">VERY</li>
                <li class="d3">LIKE / AS</li>
              </ul>
            </div>
          </main>
          <p style="font-family: 'EB Garamond'; font-size: 14px; margin-top: 4em; color: white;"><em>Natural Semantic
              Metalanguage</em>: Set of 65 semantic primes. London: Oxford University Press.<br><strong>Wierzbicka,
              A.</strong> (1996, revised in 2017). </p>
        </section>

        <!-- 3.9 Blending Theory + examples -->
        <section>
          <h3>Conceptual blending theory</h3>
          <div class="r-stack">
            <img src="svg/blending.svg" width="80%">
            <img class="fragment" src="svg/blend-1.svg" height="600">
            <img class="fragment" src="svg/blend-2.svg" height="600">
            <img class="fragment" src="svg/blend-3.svg" height="600">
          </div>
        </section>

      </section><!-- end slide 3 -->

      <!-- Separator: Part 3 - First Concepts -->
      <section data-background-color="black">
        <h1 class="divider"><span>Part 3</span><br>First Concepts</h1>
      </section>

      <!-- My definition of a Pictogram -->
      <section data-background-color="white" data-background-image="svg/definition.svg">
        <aside class="notes">A pictogram is a compossitionally structtured visual sign that encodes meaning through reduced yet recognisable graphical forms, functioning as a visual phrase capable of expressing situational or relational meaning within a given context.</aside>
      </section>

      <!-- Aesthetics of Accessibility -->
      <section class="svg-code-sync" data-svg-url="svg/pictogram.svg" data-transition="fade">
        <h2>Text as Image: <span>The Aesthetics of Accessibility</span></h2>

        <div style="display:grid;grid-template-columns:50% 50%;height:500px;gap:1rem;">
          <!-- IZQUIERDA: código -->
          <div style="overflow:auto;font-size:.55em;line-height:1.2;">
            <pre style="font-size:85%;white-space:pre;margin-left:2em;">
            <code data-role="svg-source" class="language-xml" data-line-numbers></code>
            </pre>
          </div>

          <!-- DERECHA: SVG inline -->
          <div data-role="svg-stage"
            style="display:flex;align-items:center;justify-content:center;background:#c4c2c2;border-radius:1ex;">
          </div>
        </div>
      </section>

      <!-- Separator: Part 3 - From Theory to Practice -->
      <section data-background-color="black">
        <h1 class="divider"><span>Part 3</span><br>From Theory to Practice</h1>
      </section>

      <!-- 5: Interviews and field research: AAC Cases & Interview Learnings -->
      <section id="aac-cases">
        <section data-background-image="images/oath-of-the-horatii.png">
          <div class="figure-legend" style="color: white"><strong>The Oath of the Horatii</strong>, oil on canvas by
            Jacques-Louis David, 1784.</div>
        </section>

        <section data-background-image="images/aac-bliss.png">
          <div class="figure-legend"><strong>Blissymbolics chart</strong>, selection of ideographic symbols from Charles
            K. Bliss’s system of semantography, first published in the 1949.</div>
        </section>

        <section data-background-image="images/bliss-example.jpg"></section>

        <section data-background-image="images/bliss-example-2.jpg"></section>

        <!-- PECS -->
        <section>
          <div style="text-align: center;">
            <img src="images/aac-pecs.png" alt="PECS communication book" width="90%">
          </div>
          <div class="figure-legend-img"><strong>PECS communication book</strong>, example of the Picture Exchange
            Communication System widely used in Chilean special education contexts. The system is based on the physical
            exchange of pictograms to initiate communicative acts and develop functional communication in individuals
            with autism and intellectual disabilities.</div>
        </section>

        <!-- Core Boards -->
        <section>
          <div style="text-align: center;">
            <img src="images/aac-core-board.png" alt="Core communication board" width="90%">
          </div>
          <div class="figure-legend-img"><strong>Core communication board</strong>, example of a high-frequency
            vocabulary system commonly used in New Zealand AAC practice. The board combines core words for everyday
            communication with fringe vocabulary tailored to specific activities or contexts.</div>
        </section>

      </section><!-- 5 end-->

      <!-- Separator: Part 4 - The Shape of the Proposal -->
      <section data-background-color="black">
        <h1 class="divider"><span>Part 3</span><br>The Shape of the Proposal</h1>
      </section>

      <!-- 3: PictoNet, PictoForge and MediaFranca -->
      <section>
        <!-- 3.0 - 3 layered project -->
        <section>
          <object data="svg/3-research-layers.svg" class="r-stretch"></object>
        </section>

        <!-- Sub-slide 3.1 PictoNet -->
        <section>
          <div class="col-2">
            <table class="essence-mediafranca level level-pictonet" border="0" cellpadding="8" cellspacing="0">
              <thead>
                <tr>
                  <th><span>Focus</span></th>
                  <th><strong>PictoNet</strong><br><em>Meaning through Images</em></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><em>The challenge</em></td>
                  <td>Translate communicative intention—written as text—into a pictogram that is cognitively accessible,
                    clear, and dignified.</td>
                </tr>
                <tr>
                  <td><em>Where the gap is</em></td>
                  <td>AAC still relies on illustrators and fixed libraries; systems do not yet <strong>generate</strong>
                    pictograms directly from language.</td>
                </tr>
                <tr>
                  <td><em>What to design</em></td>
                  <td>A <strong>transformer-based LLM</strong> that converts natural language into <strong>SVG</strong>
                    pictograms—using a visual grammar linking verbs, objects, and contexts.</td>
                </tr>
                <tr>
                  <td><em>Why it matters</em></td>
                  <td>Builds a bridge between text and image, expanding communication for people with complex
                    communication needs.</td>
                </tr>
              </tbody>
            </table>
            <object type="image/svg+xml" class="thumb" data="/cc/svg/level-1.svg"></object>
          </div>

          <aside class="notes">
            <p>The transformer that turns words into images. It wrestles with the old problem of representation: what
              travels from text to picture, what is lost, what can be learned.</p>
            <p>We ground this in Wittgenstein (meaning as use), Austin/Searle (speech acts), and the Natural Semantic
              Metalanguage (shared “atoms” of meaning). PictoNet maps those semantic primes to visual primitives.</p>
          </aside>
        </section>

        <!-- PictoNet Pipeline -->
        <section>
          <h2>PictoNet Pipeline</h2>
          <table class="pipeline">
            <thead>
              <tr>
                <td class="start"><span class="speech">"I want to drink water"</span></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <th><span>Step 1</span><br>Semantic Analysis (NLU)</th>
                <th><span>Step 2</span><br>Conceptual Mapping</th>
                <th><span>Step 3</span><br>Hybrid SVG Generation</th>
                <th><span>Step 4</span><br>Accessibility Post-Processing</th>
                <th>Final Pictogram Output</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  Converts text into <strong>structured meaning</strong> through <strong>speech-act
                    detection</strong>, <strong>frame semantics</strong>, and <strong>NSM decomposition</strong>.
                </td>
                <td>
                  Maps <strong>semantic roles</strong> to <strong>visual symbols</strong> using a <strong>pictogram
                    library</strong> and established <strong>graphical conventions</strong>.
                </td>
                <td>
                  Generates an <strong>SVG scaffold</strong> via <strong>LLM reasoning</strong>, refined by a
                  <strong>diffusion model</strong> for visual coherence.
                </td>
                <td>
                  Adds <strong>accessibility metadata</strong> (<strong>ARIA</strong>, <strong>WCAG</strong>) to
                  ensure compatibility with assistive technologies.
                </td>
                <td class="end">
                  <div class="center"><img src="svg/rlhf.svg" alt="learning loop" width="50%"></div>Integrates
                  <strong>user feedback</strong> through <strong>evaluation</strong> and <strong>RLHF
                    fine-tuning</strong> for iterative model improvement.
                </td>
              </tr>
            </tbody>
          </table>

        </section>

        <!-- NLU Front End-->
        <section>
          <h3>Semantic analysis <span>— NLU front-end</span></h3>
          <div class="col-2">
            <div class="say">
              <div class="speech">Please make your bed</div>
            </div>
            <pre class="nlu"><code data-trim class="language-json">
<span class="b">{</span>
<span class="k">"utterance"</span><span class="p">:</span> <span class="s">"Please make your bed"</span><span class="p">,</span>
<span class="k">"lang"</span><span class="p">:</span> <span class="s">"en"</span><span class="p">,</span>
<span class="k">"metadata"</span><span class="p">:</span> <span class="b">{</span>
  <span class="k">"speech_act"</span><span class="p">:</span> <span class="s">"directive"</span><span class="p">,</span>
  <span class="k">"intent"</span><span class="p">:</span> <span class="s">"request"</span>
<span class="b">}</span><span class="p">,</span>
<span class="k">"frames"</span><span class="p">:</span> <span class="b">[</span>
  <span class="b">{</span>
    <span class="k">"id"</span><span class="p">:</span> <span class="s">"f1"</span><span class="p">,</span>
    <span class="k">"frame_name"</span><span class="p">:</span> <span class="s">"Directed_action"</span><span class="p">,</span>
    <span class="k">"lexical_unit"</span><span class="p">:</span> <span class="s">"make"</span><span class="p">,</span>
    <span class="k">"roles"</span><span class="p">:</span> <span class="b">{</span>
      <span class="k">"Agent"</span><span class="p">:</span> <span class="b">{</span>
        <span class="k">"type"</span><span class="p">:</span> <span class="s">"Addressee"</span><span class="p">,</span>
        <span class="k">"ref"</span><span class="p">:</span> <span class="s">"you"</span><span class="p">,</span>
        <span class="k">"surface"</span><span class="p">:</span> <span class="s">"your"</span>
      <span class="b">}</span><span class="p">,</span>
      <span class="k">"Theme"</span><span class="p">:</span> <span class="b">{</span>
        <span class="k">"type"</span><span class="p">:</span> <span class="s">"Object"</span><span class="p">,</span>
        <span class="k">"lemma"</span><span class="p">:</span> <span class="s">"bed"</span><span class="p">,</span>
        <span class="k">"surface"</span><span class="p">:</span> <span class="s">"bed"</span><span class="p">,</span>
        <span class="k">"definiteness"</span><span class="p">:</span> <span class="s">"definite"</span>
      <span class="b">}</span>
    <span class="b">}</span>
  <span class="b">}</span>
<span class="b">]</span><span class="p">,</span>
<span class="k">"logical_form"</span><span class="p">:</span> <span class="b">{</span>
  <span class="k">"event"</span><span class="p">:</span> <span class="s">"make(you, bed)"</span>
<span class="b">}</span><span class="p">,</span>
<span class="k">"pragmatics"</span><span class="p">:</span> <span class="b">{</span>
  <span class="k">"politeness"</span><span class="p">:</span> <span class="s">"polite"</span><span class="p">,</span>
  <span class="k">"formality"</span><span class="p">:</span> <span class="s">"neutral"</span><span class="p">,</span>
  <span class="k">"expected_response"</span><span class="p">:</span> <span class="s">"compliance"</span>
<span class="b">}</span><span class="p">,</span>
<span class="k">"visual_guidelines"</span><span class="p">:</span> <span class="b">{</span>
  <span class="k">"focus_actor"</span><span class="p">:</span> <span class="s">"you"</span><span class="p">,</span>
  <span class="k">"action_core"</span><span class="p">:</span> <span class="s">"make"</span><span class="p">,</span>
  <span class="k">"object_core"</span><span class="p">:</span> <span class="s">"bed"</span><span class="p">,</span>
  <span class="k">"context"</span><span class="p">:</span> <span class="s">"bedroom"</span><span class="p">,</span>
  <span class="k">"temporal"</span><span class="p">:</span> <span class="s">"immediate"</span>
<span class="b">}</span>
<span class="b">}</span></code>
        </pre>
          </div>
        </section>

        <!-- Sub-slide 3.2 PictoForge-->
        <section>
          <div class="col-2">
            <table class="essence-mediafranca level level-pictoforge" border="0" cellpadding="8" cellspacing="0">
              <thead>
                <tr>
                  <th><span>Focus</span></th>
                  <th><strong>PictoForge</strong><br><em>Designing with AI</em></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><em>The challenge</em></td>
                  <td>Treat AI as a <strong>design material</strong>—directly manipulable, refineable, and
                    understandable
                    through use.</td>
                </tr>
                <tr>
                  <td><em>Where the gap is</em></td>
                  <td>Most generative tools hide inner logic; users cannot see how outputs are built or teach the system
                    how
                    to improve.</td>
                </tr>
                <tr>
                  <td><em>What to design</em></td>
                  <td>An <strong>interactive interface</strong> where users adjust pictograms and each correction
                    becomes
                    structured feedback for retraining (direct manipulation & RLHF).</td>
                </tr>
                <tr>
                  <td><em>Why it matters</em></td>
                  <td>Turns machine learning into a visible, human process—understanding grows through collaboration,
                    not
                    opacity.</td>
                </tr>
              </tbody>
            </table>
            <object type="image/svg+xml" class="thumb" data="/cc/svg/level-2.svg"></object>
          </div>

          <aside class="notes">
            <p>The workshop where people and model co-edit pictograms. Corrections, audits, and local vocabularies are
              logged as structured data. Each human nudge becomes a learning signal.</p>
            <p>This is the <strong>corrective/generative loop</strong>: propose → edit → learn. Over time the system
              internalises clarity, cultural fit, and adult dignity.</p>
          </aside>
        </section>

        <!-- PictoForge -->
        <!-- bye mockup 
        <section data-background-image="images/pictoforge.png" class="pf-slide">
          
          <img src="images/pictoforge-edit.png" alt="Segment Editor" class="fragment fade-in-then-out pf-overlay"
            style="width: 400px; margin: 0 30% 0 30%" data-fragment-index="1">
        </section> -->

        <section data-background-iframe="https://pictoforge.mediafranca.net" data-background-interactive
          data-background-opacity="1"></section>

        <!-- Sub-slide 3.3 MediaFranca -->
        <section>
          <div class="col-2">
            <table class="essence-mediafranca level level-mediafranca" border="0" cellpadding="8" cellspacing="0">
              <thead>
                <tr>
                  <th><span>Focus</span></th>
                  <th><strong>MediaFranca</strong><br><em>Language as a Commons</em></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><em>The challenge</em></td>
                  <td>Ensure the <strong>fair use of data</strong> for AI training—respecting authorship, privacy,
                    cultural identity, and model portability.</td>
                </tr>
                <tr>
                  <td><em>Where the gap is</em></td>
                  <td>Centralised infrastructures concentrate data and power; communities lose control over voices,
                    languages, and creative work.</td>
                </tr>
                <tr>
                  <td><em>What to design</em></td>
                  <td>A <strong>federated, open-source platform</strong> where local instances train their models and
                    share improvements responsibly (cultural sovereignty & ethical governance).</td>
                </tr>
                <tr>
                  <td><em>Why it matters</em></td>
                  <td>Redefines AI as a collective resource rather than a corporate asset—communities own, adapt, and
                    evolve their languages.</td>
                </tr>
              </tbody>
            </table>
            <object type="image/svg+xml" class="thumb" data="/cc/svg/level-3.svg"></object>
          </div>
          <aside class="notes">
            <p>A federated, open implementation — a <em>core communication engine</em>. Communities run local instances,
              keep their data, share model improvements. Culture stays sovereign; learning flows.</p>
            <p>Result: dialects of a shared pictographic grammar, maintained as a civic commons.</p>
          </aside>
        </section>
      </section><!-- 3 end -->

      <!-- The End: Thank You -->
      <section>
        <!-- p5 sketch of the pipeline -->
        <section data-transition="zoom">
          <h2>Thank you</h2>
          <div class="p5-host" data-sketch="pipeline" style="width:100%; height:40vh;"></div>
        </section>

        <!-- links -->
        
        <section data-background-color="#000" data-transition="slide">
          <div class="col-2">
            <div style="text-align: center;">
              <img src="svg/mediafranca-dark.svg" alt="MediaFranca Isotype" width="30%">
            </div>
            <div class="invert" style="font-size: 11px; line-height: 20px; letter-spacing: .1ex;">
              <p>The <strong>MediaFranca Initiative</strong> is a collaborative open framework for inclusive, visual, and
                linguistically grounded communication systems.</p>

              <p>Below are the core repositories that define its ecosystem:</p>

              <ol>
                <li>
                  <a href="https://github.com/mediafranca/manifesto" target="_blank"><strong>Manifiesto</strong></a> outlining the ethical, social, and design principles that guide the MediaFranca ecosystem.
                </li>
                <li>
                  <a href="https://github.com/mediafranca/pictonet" target="_blank"><strong>PictoNet</strong></a> –
                  The semantic communication network that models meanings through pictograms and linguistic structures.
                </li>
                <li>
                  <a href="https://github.com/mediafranca/pictoforge" target="_blank"><strong>PictoForge</strong></a> –
                  The pictogram editor for direct manipulation and RLHF interface. See the <a href="https://pictoforge.mediafranca.net">functional mockup</a>.
                </li>
                <li>
                  <a href="https://github.com/mediafranca/nlu-schema" target="_blank"><strong>nlu-schema</strong></a> –
                  The natural language understanding (NLU) schema defining how utterances are decomposed into semantic
                  and pragmatic structures (used by the NLU analyzer as semantic input for pictographic generation).
                </li>
                <li>
                  <a href="https://github.com/mediafranca/vcsci" target="_blank"><strong>VCSCI</strong></a> –
                  The Visual Communicability and Semantic Correspondence Index is a framework for measuring the
                  communicative adequacy of pictograms generated by a machine learning model (PictoNet).
                </li>
              </ol>
              <p><strong>Herbert Spencer González</strong> · PhD in Design (AUT)</p>
              <p><a href="https://herbertspencer.net/cc">herbertspencer.net/cc</a> · <a
                  href="mailto:herbert.spencer@autuni.ac.nz">herbert.spencer@autuni.ac.nz</a></p>
            </div>
          </div>
        </section>
        
      </section>
    </div>
  </div>


</body>

</html>