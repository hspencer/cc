<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MediaFranca</title>

  <!-- en <head> o justo antes de </body>, pero SIEMPRE antes de main.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/2.0.4/p5.min.js" defer></script>
  <script src="./p5/sketches.js" defer></script>
  <script type="module" src="svg-sync.js"></script>
</head>

<body>
  <div class="reveal">
    <div class="slides">

      <!-- 1: Introduction  -->
      <section id="intro">
        <!-- 1.1 title -->
        <section data-background-color="#FFF" data-transition="concave" id="title-slide">
          <object type="image/svg+xml" data="svg/title-illustration.svg" class="illustration"
            alt="MediaFranca Illustration: a thinking bubble of pictograms"></object>

          <div class="col-special">
            <div class="title">
              <h1><span>MediaFranca:</span>A Practice-Oriented Investigation into a Generative Pictographic System for
                Cognitive Accessibility</h1>
            </div>

            <div class="credentials">
              <div class="author">
                <dt>Candidate</dt>
                <dl><strong>Herbert Spencer González</strong><br>
                <span>PhD Student in Design</span></dl>
                
              </div>

              <div class="tutors">
                <dt>Supervisors</dt>
                <dl>Dr. Marcos Steagall</dl>
                <dl>Dr. Ivana Nakarada-Kordic</dl>
                <dt>Advisor</dt>
                <dl>Dr. Welby Ings</dl>
              </div>
            </div>
          </div>

          <div class="footer">
            <span>This presentation lives at <a href="https://herbertspencer.net/cc">herbertspencer.net/cc</a></span>
            <img class="logo" src="images/AUT.jpg" alt="Auckland University of Technology">
          </div>
        </section>

        <!-- 1.2 Aromos  -->
        <section data-background-image="images/aromos.jpg">
          <aside class="notes">This is my home in Chile, near the Aconcagua river and Cerro la Campana. I begin here
            because design is always situated; it begins from place, from the ground where we stand and from the waters
            that sustain us.</aside>
        </section>


        <!-- 1.3 Profesor  -->
        <section data-background-image="images/travesia.jpg">
          <h2 class="photo-overlay"><span>Travesía 2018</span><br><span>Corredor Litoral</span><br><span
              class="light">First Year Design Studio</span><span>+</span><br><span class="light">Third year Light and
              Color Design Studio</span></h2>
          <aside class="notes">Here you see my students during a design "Travesía" in the north of Chile. We travelled
            where the Pacific meets the Atacama desert and built, at full scale, a timber “coastal corridor”. For me,
            teaching design has always meant expanding perception, opening encounters, and making space for others to
            participate.</aside>
        </section>

        <!-- Núcleo de Investigación en Accesibilidad e Inclusión PUCV  -->
        <section data-background-image="images/nucleo.jpg">
          <h2 class="photo-overlay"><span class="light">Research Centre for</span><br><span>Accessibility and
              Inclusion</span><br><span class="light">Pontifical Catholic University of Valparaíso</span></h2>
          <aside class="notes">
            Over time, the team grew into an interdisciplinary centre, with people from linguistics, engineering,
            sociology, law and more. Together we tried to build knowledge across differences, following the idea that
            inclusive research requires inclusive teams.
          </aside>
        </section>

        <!-- 1.5 PICTOS Service  -->
        <section class="w">
          <img class="fit" src="images/pictos-service.png" alt="PICTOS Sevice Arqchitecture">
          <aside class="notes">
            So we designed supports: a grammar for pictograms, a system for plain language, and a publishing
            architecture to adapt these supports across services. The aim was not to simplify life but to return
            dignity, to enable people to complete everyday transactions without dependence... and get the value that
            these services offer.
          </aside>
        </section>

        <!-- the space between words and images -->
        <section data-background-color="#FFF" data-background-image="svg/word-visual-space.svg">
          <aside class="notes">Both groups converge here: they say cognitive accessibility depends on coherence between
            words and images. Therapists describe the fatigue of their students when facing mismatched systems.
            Designers describe the failure of icons when they drift into opacity or cliché. Their insights confirm the
            hypothesis: the corridor between word and image must remain tight, structured, and open to correction.
            The PICTOS approach consists of a highly coherent support between the word space and the image. The direct
            correspondence between instruction and pictogram promotes comprehension and reduces cognitive overload. This
            is the fundamental idea on which my research is based.</aside>
        </section>

        <!-- 1.3 research question-->
        <section>
          <p class="smallcaps">research question:</p>
          <p class="huge question">
            How can a generative pictographic system <span>be designed</span> to support
            communication for people with complex communication needs?
          </p>
          <aside class="notes">The core question I bring here is: how can a generative pictographic system be designed
            to support communication for people with complex communication needs? To approach this, I am not working
            alone in my studio. I am conducting interviews with two groups of professionals. On the one hand, those who
            teach, educate, and promote the use of AAC tools in schools, clinics, and daily practices. On the other
            hand, designers who have dedicated their craft to pictograms with great concern for accessibility,
            comprehension, synthesis, and elegance. Their voices frame the territory of this research.</aside>
        </section>
      </section>

      <!-- 2: Why this research matters -->
      <section id="why-matters">
        <!-- 2.1 Scale of need (Aotearoa NZ) -->
        <section data-background-color="#FFF">
          <h2 class="xl">Why this research matters?</h2>
          <div class="image">
            <img src="svg/people-with-ccn.svg" alt="between 6% and 10% have complex communication needs">
            <div class="legend">
              <ul>
                <li><strong>1 in 6</strong> New Zealanders (≈ 851,000 people) live with disability (2023 Census).</li>
                <li><strong>5.9%</strong> of people aged 5+ report some difficulty communicating.</li>
                <li>Professional estimates suggest <strong>up to 10%</strong> experience communication difficulties.</li>
              </ul>
            </div>
          </div>

          <aside class="notes">
            Land the scale: 17% disabled (851k). Communication difficulty: 5.9% Census; up to 10% (SLT NZ) — frame as
            range to be honest about methods. Emphasise quality of life and participation.
          </aside>
        </section>

        <!-- 2.2 What AAC is (plain) -->
        <section data-background-image="images/aac-systems.png"  data-transition="fade" class="invert">
          <h2 class="xl">What is AAC?</h2>
          <div class="image">
            <img src="images/ccn.png" alt="Pictogram of a person with complex communication needs"
              style="width: 300px;">
            <p><strong>Augmentative and Alternative Communication (AAC)</strong> means extra ways to be heard — signs,
              symbols, picture boards, apps, eye-gaze devices, and speech-generating tech.</p>
            <p>It doesn’t replace speech; it <em>adds channels</em> so people can say what they mean.</p>
          </div>

          <aside class="notes">
            Keep it friendly: “extra ways to be heard.” Stress dignity and agency.

            Truth #2: AAC is not only for non-speaking individuals
            Yes, you read that correctly. AAC can benefit those that speak. You do not have to have a label of
            “nonverbal” or “non-speaking” or “minimally speaking” to benefit from AAC.

            If you can’t speak in an emergency, you might benefit from AAC.
            If you can’t speak when dysregulated, you might benefit from AAC.
            If your speech is unclear to familiar or unfamiliar listeners, you might benefit from AAC.
            If you are having communication breakdowns and showing signs of frustration, you might benefit from AAC
            If you have trouble initiating speech without a model, you might benefit from AAC.
          </aside>
        </section>

        <!-- 2.3 A gap in representation -->
        <section data-transition="fade">
          <h2 class="xl">A perssitent gap</h2>
          <table class="gap">
            <thead>
              <tr>
                <th><img src="svg/noun-003.svg"><br>Speech & Language Augmentation</th>
                <th><img src="svg/noun-004.svg"><br>Predictive Communication Systems</th>
                <th><img src="svg/noun-002.svg"><br>Embodied / Sensor-Based Interfaces</th>
                <th><img src="svg/noun-001.svg"><br>Pictographic & Symbolic Representation</th>
              </tr>
            </thead>
            <tbody>
              <td>
                <ul>
                  <li class="check">voice recognition</li>
                  <li class="check">text-to-speech synthesis</li>
                  <li class="check">speech-to-text captioning</li>
                  <li class="check">AI speech repair for dysarthria</li>
                </ul>
              </td>
              <td>
                <ul>
                  <li class="check">predictive text</li>
                  <li class="check">next pictogram prediction</li>
                  <li class="check">context-aware phrase suggestion</li>
                  <li class="check">adaptive keyboards</li>
                </ul>
              </td>
              <td>
                <ul>
                  <li class="check">swipe keyboards</li>
                  <li class="check">eye tracking</li>
                  <li class="check">gesture control</li>
                  <li class="check">facial expression detection</li>
                  <li class="check">EEG-based intent sensing</li>
                </ul>
              </td>
              <td>
                <ul>
                  <li class="check">static pictogram sets</li>
                  <li class="fail">no dynamic generative pictographic systems</li>
                </ul>
              </td>
            </tbody>
          </table>
        </section>
      </section><!-- 2 end -->

      <!-- 3: PictoNet, PictoForge and MediaFranca -->
      <section>
        <!-- 3.0 - 3 layered project -->
        <section>
          <h2>The Shape of the Proposal</h2>
          <object data="svg/3-research-layers.svg" class="r-stretch"></object>
          <aside class="notes">So, this research is structured across three layers. At the first level, the pictogram
            itself: what makes it good, clear, and memorable — as AAC professionals say, “learnable and consistent,” and
            as designers add, “synthetic and elegant.” At the second level, the interaction: how humans and generative
            systems co-compose, how reversibility and transparency are guaranteed, how edits remain explainable. At the
            third level, the infrastructure: how corrections become collective, how cultural adaptations are federated,
            and how governance prevents decay into noise. This threefold structure is not only my method; it is also my
            ethical stance.</aside>
        </section>

        <!-- Sub-slide 3.1 PictoNet -->
        <section>
          <div class="col-2">
            <table class="essence-mediafranca level level-pictonet" border="0" cellpadding="8" cellspacing="0">
              <thead>
                <tr>
                  <th><span>Focus</span></th>
                  <th><strong>PictoNet</strong><br><em>Meaning through Images</em></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><em>The challenge</em></td>
                  <td>Translate communicative intention—written as text—into a pictogram that is cognitively accessible,
                    clear, and dignified.</td>
                </tr>
                <tr>
                  <td><em>Where the gap is</em></td>
                  <td>AAC still relies on illustrators and fixed libraries; systems do not yet <strong>generate</strong>
                    pictograms directly from language.</td>
                </tr>
                <tr>
                  <td><em>What to design</em></td>
                  <td>A <strong>transformer-based LLM</strong> that converts natural language into <strong>SVG</strong>
                    pictograms—using a visual grammar linking verbs, objects, and contexts.</td>
                </tr>
                <tr>
                  <td><em>Why it matters</em></td>
                  <td>Builds a bridge between text and image, expanding communication for people with complex
                    communication needs.</td>
                </tr>
              </tbody>
            </table>
            <object type="image/svg+xml" class="thumb" data="/svg/level-1.svg"></object>
          </div>

          <aside class="notes">
            <p>The transformer that turns words into images. It wrestles with the old problem of representation: what
              travels from text to picture, what is lost, what can be learned.</p>
            <p>We ground this in Wittgenstein (meaning as use), Austin/Searle (speech acts), and the Natural Semantic
              Metalanguage (shared “atoms” of meaning). PictoNet maps those semantic primes to visual primitives.</p>
          </aside>

        </section>

        <!-- Sub-slide 3.2 PictoForge-->
        <section>
          <div class="col-2">
            <table class="essence-mediafranca level level-pictoforge" border="0" cellpadding="8" cellspacing="0">
              <thead>
                <tr>
                  <th><span>Focus</span></th>
                  <th><strong>PictoForge</strong><br><em>Designing with AI</em></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><em>The challenge</em></td>
                  <td>Treat AI as a <strong>design material</strong>—directly manipulable, refineable, and
                    understandable
                    through use.</td>
                </tr>
                <tr>
                  <td><em>Where the gap is</em></td>
                  <td>Most generative tools hide inner logic; users cannot see how outputs are built or teach the system
                    how
                    to improve.</td>
                </tr>
                <tr>
                  <td><em>What to design</em></td>
                  <td>An <strong>interactive interface</strong> where users adjust pictograms and each correction
                    becomes
                    structured feedback for retraining (direct manipulation & RLHF).</td>
                </tr>
                <tr>
                  <td><em>Why it matters</em></td>
                  <td>Turns machine learning into a visible, human process—understanding grows through collaboration,
                    not
                    opacity.</td>
                </tr>
              </tbody>
            </table>
            <object type="image/svg+xml" class="thumb" data="/svg/level-2.svg"></object>
          </div>

          <aside class="notes">
            <p>The workshop where people and model co-edit pictograms. Corrections, audits, and local vocabularies are
              logged as structured data. Each human nudge becomes a learning signal.</p>
            <p>This is the <strong>corrective/generative loop</strong>: propose → edit → learn. Over time the system
              internalises clarity, cultural fit, and adult dignity.</p>
          </aside>

        </section>

        <!-- Sub-slide 3.3 MediaFranca -->
        <section>
          <div class="col-2">
            <table class="essence-mediafranca level level-mediafranca" border="0" cellpadding="8" cellspacing="0">
              <thead>
                <tr>
                  <th><span>Focus</span></th>
                  <th><strong>MediaFranca</strong><br><em>Language as a Commons</em></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><em>The challenge</em></td>
                  <td>Ensure the <strong>fair use of data</strong> for AI training—respecting authorship, privacy,
                    cultural identity, and model portability.</td>
                </tr>
                <tr>
                  <td><em>Where the gap is</em></td>
                  <td>Centralised infrastructures concentrate data and power; communities lose control over voices,
                    languages, and creative work.</td>
                </tr>
                <tr>
                  <td><em>What to design</em></td>
                  <td>A <strong>federated, open-source platform</strong> where local instances train their models and
                    share improvements responsibly (cultural sovereignty & ethical governance).</td>
                </tr>
                <tr>
                  <td><em>Why it matters</em></td>
                  <td>Redefines AI as a collective resource rather than a corporate asset—communities own, adapt, and
                    evolve their languages.</td>
                </tr>
              </tbody>
            </table>
            <object type="image/svg+xml" class="thumb" data="/svg/level-3.svg"></object>
          </div>

          <aside class="notes">
            <p>A federated, open implementation — a <em>core communication engine</em>. Communities run local instances,
              keep their data, share model improvements. Culture stays sovereign; learning flows.</p>
            <p>Result: dialects of a shared pictographic grammar, maintained as a civic commons.</p>
          </aside>

        </section>
      </section>

      <!-- 4: The Problem of Language -->
      <section id="problem-of-language">
        
        <section data-background-image="images/babel.jpg">
          <h2 class="huge invert">The problem of language</h2>
          <aside class="notes">
            Opens the final conceptual arc: from linguistic purity to pragmatic use.
          </aside>
        </section>

        <!-- 4.1 -->
        <section data-background-image="images/characteristica_universalis.jpg" class="invert">
          <br><br><br><br><br><br><br><br>
          <br><br><br><br><br><br><br><br>
          <h3>The search for the perfect and universal</h3>

          <aside class="notes">
            <p>For centuries, philosophers dreamed of a language so precise that misunderstanding would be impossible —
              universal, timeless, context-free.</p>
            <p>From Plato to Leibniz and Frege, clarity grew as life receded. A sealed code travels well — but breathes
              poorly.</p>
          </aside>
        </section>

        <!-- 4.2 -->
        <section id="situated-language" data-transition="fade">
          <h3>The complexity of the situated</h3>
          <div class="image">
            <img src="images/img-06.png" alt="A pictographic explanation of the same event in English and Nootka">
          </div>

          <p>Meaning lives in practice. Words gain sense from use, activity, and timing.</p>
          <p>Language games (Wittgenstein) and linguistic relativity (Sapir–Whorf) show that communities shape what
            things mean.</p>
        </section>

        <!-- 4.3 -->
         <section data-background-image='images/relativity.jpg'></section>

        <!-- Aesthetics of Accessibility -->
        <!-- 4.3.1 Primera instancia -->
        <section class="svg-code-sync" data-svg-url="svg/pictogram.svg" data-transition="fade">
          <h2>Text as Image: <span>The Aesthetics of Accessibility</span></h2>

          <div style="display:grid;grid-template-columns:50% 50%;height:500px;gap:1rem;">
            <!-- IZQUIERDA: código -->
            <div style="overflow:auto;font-size:.55em;line-height:1.2;">
              <pre style="font-size:85%;white-space:pre;margin-left:2em;">
              <code data-role="svg-source" class="language-xml" data-line-numbers></code>
              </pre>
            </div>

            <!-- DERECHA: SVG inline -->
            <div data-role="svg-stage"
              style="display:flex;align-items:center;justify-content:center;background:#c4c2c2;border-radius:1ex;">
            </div>
          </div>
        </section>
        
        <!-- 4.3.2 Segunda instancia -->
        <section class="svg-code-sync" data-svg-url="svg/ccn.svg" data-transition="fade">
          <h2>Direct <span>Manipulation</span></h2>

          <div style="display:grid;grid-template-columns:50% 50%;height:500px;gap:1rem;">
            <!-- IZQUIERDA: código -->
            <div style="overflow:auto;font-size:.55em;line-height:1.2;">
              <pre style="font-size:85%;white-space:pre;margin-left:2em;">
              <code data-role="svg-source" class="language-xml" data-line-numbers></code>
              </pre>
            </div>

            <!-- DERECHA: SVG inline -->
            <div data-role="svg-stage"
              style="display:flex;align-items:center;justify-content:center;background:#9c9b9b;border-radius:1ex;">
            </div>
          </div>
        </section>

      </section><!-- end slide 4 -->

      <!-- 5: AAC Cases & Interview Learnings -->
      <section id="aac-cases">
        <section>
          <h2>AAC cases and interview learnings</h2>
        </section>

        <section>
          <h3>Blissymbolics</h3>
          <p>Ambition for perfect order — logical, complete. In practice: coherent yet brittle; too rigid for messy
            life.</p>
        </section>

        <section data-transition="fade">
          <h3>PECS (Picture Exchange Communication System)</h3>
          <p>Communication as action: cards as tokens of agency. Powerful for a first bridge, yet expression may stall
            in repetition.</p>
        </section>

        <section data-transition="fade">
          <h3>Core boards and LAMP</h3>
          <p>Fixed icon locations shift language into movement; the body learns paths. Powerful, yet demanding of
            stability and context.</p>
        </section>

        <section data-background-color="#FFF">
          <h3>What this implies for PictoNet</h3>
          <ul>
            <li><strong>Pragmatics-aware:</strong> recognise speech-acts (request, comment, refusal).</li>
            <li><strong>Abstraction ladder:</strong> move between concrete and abstract with learnable conventions.</li>
            <li><strong>Culturally adaptable:</strong> shared grammar, local exponents.</li>
            <li><strong>Co-created:</strong> corrections and styles from users and professionals.</li>
            <li><strong>Hybrid:</strong> works across screen and print, high-tech and low-tech.</li>
          </ul>
        </section>
      </section>

      <!-- 6: Designing PictoNet (the Pipeline) -->
      <section id="pipeline">
        <section>
          <h2>Designing PictoNet (the pipeline)</h2>
          <div class="p5-host" data-sketch="pipeline" style="width:100%; height:40vh;"></div>
          <ol class="inline">
            <li>Understand the utterance</li>
            <li>Map meaning to visual parts</li>
            <li>Generate a clean SVG</li>
            <li>Add accessibility metadata</li>
            <li>Learn from human edits (loop)</li>
          </ol>
        </section>

        <section>
          <h3>1 · Semantic analysis — NLU front-end</h3>
          <code lang="javascript" class="hljs">
            {
              "utterance": "Please make your bed",
              "lang": "en",
              "metadata": { "speech_act": "directive", "intent": "request" },
              "frames": [
                {
                  "id": "f1",
                  "frame_name": "Directed_action",
                  "lexical_unit": "make",
                  "roles": {
                    "Agent": { "type": "Addressee", "ref": "you", "surface": "your" },
                    "Theme": { "type": "Object", "lemma": "bed", "surface": "bed", "definiteness": "definite" }
                  }
                }
              ],
              "logical_form": { "event": "make(you, bed)" },
              "pragmatics": { "politeness": "polite", "formality": "neutral", "expected_response": "compliance" },
              "visual_guidelines": { "focus_actor": "you", "action_core": "make", "object_core": "bed", "context": "bedroom", "temporal": "immediate" }
            }
          </code>
          <ul>
            <li>Extract roles: who does what, to whom, with what intent.</li>
            <li>Classify speech-act: directive, question, statement, social.</li>
            <li>Decompose abstract terms using NSM primitives.</li>
          </ul>
        </section>

        <section data-transition="fade">
          <h3>2 · Concept mapping → visual primitives</h3>
          <ul>
            <li>Link roles to a <em>Pictogram Concept Set</em> (IDs + relations).</li>
            <li>Apply conventions: clouds = intangible; motion marks = verbs; plural marks.</li>
            <li>Use local style dictionaries for cultural fit.</li>
          </ul>
        </section>

        <section data-transition="fade">
          <h3>3 · Hybrid SVG generation</h3>
          <ul>
            <li>Produce structured SVG (groups/IDs ↔ semantics).</li>
            <li>Refine proportion and balance algorithmically.</li>
            <li>Editable, inspectable artefacts instead of opaque bitmaps.</li>
          </ul>
        </section>

        <section data-transition="fade">
          <h3>4 · Accessibility post-processing</h3>
          <ul>
            <li>Add title/desc, ARIA roles, semantic tags.</li>
            <li>Export for print boards and web use.</li>
            <li>One asset, many contexts.</li>
          </ul>
        </section>

        <section data-transition="fade">
          <h3>5 · Human-in-the-loop refinement</h3>
          <ul>
            <li>PictoForge captures edits as structured feedback.</li>
            <li>Corrections guide retraining (RLHF) and local style embeddings.</li>
            <li>Federated updates share improvements safely.</li>
          </ul>
        </section>

        <section data-background-color="#FFF">
          <h3>What this could enable</h3>
          <ul>
            <li>New pictographic vocabulary on demand.</li>
            <li>Visible “why” behind each drawing.</li>
            <li>High-tech ↔ low-tech continuity.</li>
            <li>Cultural adaptation within shared structure.</li>
          </ul>
        </section>
      </section>

      <!-- 7: The Invitation -->
      <section id="invitation">
        <section>
          <h2>The invitation</h2>
          <p>This work invites collaboration on:</p>
          <ul>
            <li>Pictogram validation (comprehension, dignity, context).</li>
            <li>Semantic-adhesion metrics (intent ↔ image fit).</li>
            <li>Interfaces for AI as design material (direct manipulation, auditability).</li>
            <li>Local style embeddings and federated learning.</li>
          </ul>
        </section>
      </section>

      <!-- Thank You -->
      <section id="thanks" data-background-color="#FFF">
        <h2>Thank you</h2>

        <p>
          The <strong>MediaFranca Initiative</strong> is a collaborative open framework for inclusive, visual, and
          linguistically grounded communication systems.
          Below are the core repositories that define its ecosystem, each focusing on a different layer of the pipeline:
        </p>

        <ol>
          <li>
            <a href="https://github.com/mediafranca/manifesto" target="_blank"><strong>manifiesto</strong></a> –
            The foundational document outlining the ethical, social, and design principles that guide the MediaFranca
            ecosystem.
          </li>
          <li>
            <a href="https://github.com/mediafranca/pictonet" target="_blank"><strong>pictonet</strong></a> –
            The semantic communication network that models meanings through pictograms and linguistic structures.
          </li>
          <li>
            <a href="https://github.com/mediafranca/pictoforge" target="_blank"><strong>pictoforge</strong></a> –
            The visual synthesis engine that generates pictographic messages from structured semantic representations.
          </li>
          <li>
            <a href="https://github.com/mediafranca/nlu-schema" target="_blank"><strong>nlu-schema</strong></a> –
            The natural language understanding (NLU) schema defining how utterances are decomposed into semantic and
            pragmatic structures (used by the NLU analyzer).
          </li>
          <li>
            <a href="https://github.com/mediafranca/vcsci" target="_blank"><strong>vcsci</strong></a> –
            The Visual-Semantic Communication Interface that connects linguistic understanding with visual expression
            for accessible communication.
          </li>
        </ol>
        <p><strong>Herbert Spencer González</strong> · PhD in Design (AUT)</p>
        <p><a href="https://herbertspencer.net/cc">herbertspencer.net/cc</a> · <a
            href="mailto:herbert.spencer@autuni.ac.nz">herbert.spencer@autuni.ac.nz</a></p>
      </section>


    </div>
  </div>

  <script type="module" src="./main.js"></script>

</body>

</html>