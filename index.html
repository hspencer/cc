<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MediaFranca</title>

  <!-- en <head> o justo antes de </body>, pero SIEMPRE antes de main.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/2.0.4/p5.min.js" defer></script>
  <script src="./p5/sketches.js" defer></script>
</head>

<body>
  <div class="reveal">
    <div class="slides">

      <!-- 1: Title & About me  -->
      <section>
        <!-- 1.1 title -->
        <section data-background-color="#FFF" data-transition="concave" id="title-slide">
          <object type="image/svg+xml" data="svg/title-illustration.svg" class="illustration"
            alt="MediaFranca Illustration: a thinking bubble of pictograms"></object>

          <div class="col-2">
            <div class="title">
              <h1><span>MediaFranca:</span>A Practice-Oriented Investigation into a Generative Pictographic System for
                Cognitive Accessibility</h1>
            </div>

            <div class="credentials">
              <div class="author">
                <strong>Herbert Spencer González</strong><br>
                PhD Student in Design
              </div>

              <div class="tutors">
                <dt>Supervisors</dt>
                <dl>Dr. Marcos Steagall</dl>
                <dl>Dr. Ivana Nakarada-Kordic</dl>
                <dt>Advisor</dt>
                <dl>Dr. Welby Ings</dl>
              </div>
            </div>
          </div>

          <div class="footer">
            <span>This presentation lives at <a href="https://herbertspencer.net/cc">herbertspencer.net/cc</a></span>
            <img class="logo" src="images/AUT.jpg" alt="Auckland University of Technology">
          </div>


        </section>

        <!-- 1.2 Aromos  -->
        <section data-background-image="images/aromos.jpg">
          <aside class="notes">This is my home in Chile, near the Aconcagua river and Cerro la Campana. I begin here
            because design is always situated; it begins from place, from the ground where we stand and from the waters
            that sustain us.</aside>
        </section>

        <!-- 1.3 wiki Casiopea -->
        <!-- <section data-background-iframe="https://wiki.ead.pucv.cl/Herbert_Spencer" data-background-interactive
          data-background-opacity="1">
        </section> -->

        <!-- 1.3 Profesor  -->
        <section data-background-image="images/travesia.jpg">
          <h2 class="photo-overlay"><span>Travesía 2018</span><br><span>Corredor Litoral</span><br><span
              class="light">First Year Design Studio</span><span>+</span><br><span class="light">Third year Light and
              Color Design Studio</span></h2>
          <aside class="notes">Here you see my students during a design "Travesía" in the north of Chile. We travelled
            where the Pacific meets the Atacama desert and built, at full scale, a timber “coastal corridor”. For me,
            teaching design has always meant expanding perception, opening encounters, and making space for others to
            participate.</aside>
        </section>

        <!-- Núcleo de Investigación en Accesibilidad e Inclusión PUCV  -->
        <section data-background-image="images/nucleo.jpg" >
          <h2 class="photo-overlay"><span class="light">Research Centre for</span><br><span>Accessibility and Inclusion</span><br><span class="light">Pontifical Catholic University of Valparaíso</span></h2>
          <aside class="notes">
            Over time, the team grew into an interdisciplinary centre, with people from linguistics, engineering, sociology, law and more. Together we tried to build knowledge across differences, following the idea that inclusive research requires inclusive teams.
          </aside></aside>
        </section>

        <!-- 1.5 PICTOS Service  -->
        <section class="w">
          <img class="fit" src="images/pictos-service.png" alt="PICTOS Sevice Arqchitecture">
          <aside class="notes">
            So we designed supports: a grammar for pictograms, a system for plain language, and a publishing
            architecture to adapt these supports across services. The aim was not to simplify life but to return
            dignity, to enable people to complete everyday transactions without dependence... and get the value that
            these services offer.
          </aside>
        </section>

        <!-- 1.6 Pictogramas de Pictos -->
        <section data-background-iframe="https://pictogramas.pictos.cl" data-background-interactive
          data-background-opacity="1" class="zoom125">
          <aside class="notes">
            pictograms are constructed as the sum of 3 layers. The complete support is the pictogram that
            represents the situation, the instruction in writen and spoken word and an icon that reinforces the action.
            From the designers I interviewed, a different concern emerges. They insist that a good pictogram is not just
            guessable; it must hold clarity without losing dignity. They care about stroke weight, proportion, and
            balance, because these small decisions affect legibility and trust. One told me, “elegance is not
            decoration; it is what keeps the image memorable.” That resonates with the PICTOS approach, where we built
            supports in layers to reduce cognitive load.
          </aside>
        </section> 

        <!-- the space between words and images -->
         
        <section data-background-color="#FFF" data-background-image="svg/word-visual-space.svg">
          <aside class="notes">Both groups converge here: they say cognitive accessibility depends on coherence between
            words and images. Therapists describe the fatigue of their students when facing mismatched systems.
            Designers describe the failure of icons when they drift into opacity or cliché. Their insights confirm the
            hypothesis: the corridor between word and image must remain tight, structured, and open to correction.
            The PICTOS approach consists of a highly coherent support between the word space and the image. The direct
            correspondence between instruction and pictogram promotes comprehension and reduces cognitive overload. This
            is the fundamental idea on which my research is based.</aside>
        </section>
        

        <!-- 1.3 research question-->
        <section>
          <p class="smallcaps">research question:</p>
          <p class="huge question">
            How can a generative pictographic system <span>be designed</span> to support
            communication for people with complex communication needs?
          </p>
          <aside class="notes">The core question I bring here is: how can a generative pictographic system be designed
            to support communication for people with complex communication needs? To approach this, I am not working
            alone in my studio. I am conducting interviews with two groups of professionals. On the one hand, those who
            teach, educate, and promote the use of AAC tools in schools, clinics, and daily practices. On the other
            hand, designers who have dedicated their craft to pictograms with great concern for accessibility,
            comprehension, synthesis, and elegance. Their voices frame the territory of this research.</aside>
        </section>
      </section>




      <!-- Slide 2 -->
      <!-- 2: Why this research matters -->
      <section id="why-matters">

        <!-- 2.1 Scale of need (Aotearoa NZ) -->
        <section data-background-color="#FFF">
          <h3 class="xl">Why this research matters?</h3>
          <div class="image">
            <img src="svg/people-with-ccn.svg" alt="between 6% and 10% have complex communication needs" style="margin 0 auto; width: 60vw;">
            <div class="legend">
            In Aotearoa New Zealand, about <strong>1 in 6 people</strong> live with disability — around
            <strong>851,000</strong> people in 2023. And communication is a big part of that picture: the Census shows about <strong>5.9%</strong> of people
            aged 5+ report some difficulty communicating; professional estimates put <strong>up to 10%</strong> of New
            Zealanders with a communication difficulty. Behind every number is everyday life: friendships, school, work, independence.
          </div>
          </div>

          <aside class="notes">
            Land the scale: 17% disabled (851k). Communication difficulty: 5.9% Census; up to 10% (SLT NZ) — frame as
            range to be honest about methods. Emphasise quality of life and participation.
          </aside>
        </section>

        <!-- 2.2 What AAC is (plain) -->
        <section data-transition="fade">
          <h3>What is AAC?</h3>
          <img src="images/ccn.png" alt="Pictogram of a person with complex communication needs" style="width: 300px;">
          <p><strong>Augmentative and Alternative Communication (AAC)</strong> means extra ways to be heard — signs,
            symbols, picture boards, apps, eye-gaze devices, and speech-generating tech.</p>
          <p>It doesn’t replace speech; it <em>adds channels</em> so people can say what they mean.</p>
          <aside class="notes">
            Keep it friendly: “extra ways to be heard.” Stress dignity and agency.

            Truth #2: AAC is not only for non-speaking individuals
            Yes, you read that correctly. AAC can benefit those that speak.  You do not have to have a label of “nonverbal” or “non-speaking” or “minimally speaking” to benefit from AAC.

            If you can’t speak in an emergency, you might benefit from AAC.
            If you can’t speak when dysregulated, you might benefit from AAC.
            If your speech is unclear to familiar or unfamiliar listeners, you might benefit from AAC.
            If you are having communication breakdowns and showing signs of frustration, you might benefit from AAC
            If you have trouble initiating speech without a model, you might benefit from AAC.
          </aside>
        </section>

        <!-- 2.3 What's improved already -->
        <section data-transition="fade">
          <h4>What’s improved already</h4>
          <ol>
            <li>Better <strong>text-to-speech</strong> and <strong>speech-to-text</strong>.</li>
            <li>Smarter typing and <strong>next-word</strong> prediction.</li>
            <li>Eye-tracking and access tech for complex physical needs.</li>
            <li>Richer symbol sets and classroom practices.</li>
          </ol>
          <p>All of this helps — it lowers effort and speeds up conversation.</p>
          <aside class="notes">
            Acknowledge real progress so the audience trusts the gap we name next.
          </aside>
        </section>

        <!-- 2.4 The stubborn gap: representation -->
        <section data-transition="fade">
          <h4>The stubborn gap</h4>
          <p>But one piece hasn’t moved enough: <strong>representation</strong> — how ideas become pictures.</p>
          <p>Symbol vocabularies are still <em>limited</em>, sometimes <em>inconsistent</em>, and often
            <em>infantilising</em>.
          </p>
          <p>Teachers tell us about the same moment again and again: a student knows exactly what they want to say — and
            can’t find the symbol. The moment passes. The feeling doesn’t.</p>
          <aside class="notes">
            Tell the story: searching for the “right” pictogram and not finding it. Make the pain point human.
          </aside>
        </section>

        <!-- 2.5 A human story (PECS as currency) -->
        <section data-transition="fade">
          <h4>When symbols become currency</h4>
          <p>In PECS, children sometimes <em>hide</em> the “cake” card to use it later. The card isn’t a picture of cake
            — it’s a <strong>token of agency</strong>.</p>
          <p>That’s the message: language is action. Our systems must meet that reality.</p>
          <aside class="notes">
            Short, vivid anecdote to bridge into “speech-act → pictogram”.
          </aside>
        </section>

        <!-- 2.6 The opportunity: intent → pictogram -->
        <section data-transition="fade">
          <h4>A new opportunity</h4>
          <p>The next frontier is <strong>automating the step from intent to image</strong> — from the <em>speech
              act</em> (“I want water”, “No thanks”, “That’s funny”) to a clear, adult, culturally fitting pictogram.
          </p>
          <p>This niche is <strong>under-explored</strong> in today’s generative AI. That’s where our project lives.</p>
          <p><strong>PictoNet</strong> aims to learn this mapping; <strong>PictoForge</strong> lets people correct it;
            <strong>MediaFranca</strong> shares those improvements across communities.
          </p>
          <aside class="notes">
            Set up the three-layer solution you’ll unpack next. Keep the promise tangible: clearer, adult, culturally
            fitting pictograms on demand.
          </aside>
        </section>

        <!-- optional footer with tiny citations for your own reference -->
        <section data-visibility="hidden">
          <small>
            Sources (for speaker reference): Stats NZ Disability statistics 2023; Whaikaha summary; 2023 Census
            communication difficulty; NZSLT Data Fact Sheet; MoE Inclusive Education AAC.
          </small>
        </section>

      </section>


      <!-- Slide 3, the 3 layers: PictoNet, PictoForge and MediaFranca -->
      <section>
        
        <!-- 3.1 - 3 layered project -->
        <section>
          <object data="svg/3-research-layers.svg" class="r-stretch"></object>
          <aside class="notes">So, this research is structured across three layers. At the first level, the pictogram itself: what makes it good, clear, and memorable — as AAC professionals say, “learnable and consistent,” and as designers add, “synthetic and elegant.” At the second level, the interaction: how humans and generative systems co-compose, how reversibility and transparency are guaranteed, how edits remain explainable. At the third level, the infrastructure: how corrections become collective, how cultural adaptations are federated, and how governance prevents decay into noise. This threefold structure is not only my method; it is also my ethical stance.</aside>
        </section>

        <!-- tabla completa 
        <section>
          <table class="essence-mediafranca" border="0" cellpadding="8" cellspacing="0">
            <thead>
              <tr>
                <th><span>Focus</span></th>
                <th><strong>PictoNet</strong><br><em>Meaning through Images</em></th>
                <th><strong>PictoForge</strong><br><em>Designing with AI</em></th>
                <th><strong>MediaFranca</strong><br><em>Language as a Commons</em></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><em>The challenge</em></td>
                <td>Translate communicative intention — written as text — into a pictogram that is cognitively
                  accessible, clear, and dignified.</td>
                <td>Treat AI as a <strong>design material</strong>—directly manipulable, refineable, and understandable
                  through use.</td>
                <td>Ensure the <strong>fair use of data</strong> for AI training—respecting authorship, privacy,
                  cultural identity, and model portability.</td>
              </tr>
              <tr>
                <td><em>Where the gap is</em></td>
                <td>AAC still relies on illustrators and fixed libraries; systems do not yet <strong>generate</strong>
                  pictograms directly from language.</td>
                <td>Most generative tools hide inner logic; users cannot see how outputs are built or teach the system
                  how to improve.</td>
                <td>Centralised infrastructures concentrate data and power; communities lose control over voices,
                  languages, and creative work.</td>
              </tr>
              <tr>
                <td><em>What to design</em></td>
                <td>A <strong>transformer-based LLM</strong> that converts natural language into <strong>SVG</strong>
                  pictograms—using a visual grammar linking verbs, objects, and contexts.</td>
                <td>An <strong>interactive interface</strong> where users adjust pictograms and each correction becomes
                  structured feedback for retraining (direct manipulation & RLHF).</td>
                <td>A <strong>federated, open-source platform</strong> where local instances train their models and
                  share improvements responsibly (cultural sovereignty & ethical governance).</td>
              </tr>
              <tr>
                <td><em>Why it matters</em></td>
                <td>Builds a bridge between text and image, expanding communication for people with complex
                  communication needs.</td>
                <td>Turns machine learning into a visible, human process—understanding grows through collaboration, not
                  opacity.</td>
                <td>Redefines AI as a collective resource rather than a corporate asset—communities own, adapt, and
                  evolve their languages.</td>
              </tr>
            </tbody>
          </table>
        </section>
        -->

        <!-- Sub-slide 3.1 -->
        <section>
          <div class="col-2">
            <table class="essence-mediafranca level level-pictonet" border="0" cellpadding="8" cellspacing="0">
            <thead>
              <tr>
                <th><span>Focus</span></th>
                <th><strong>PictoNet</strong><br><em>Meaning through Images</em></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><em>The challenge</em></td>
                <td>Translate communicative intention—written as text—into a pictogram that is cognitively accessible,
                  clear, and dignified.</td>
              </tr>
              <tr>
                <td><em>Where the gap is</em></td>
                <td>AAC still relies on illustrators and fixed libraries; systems do not yet <strong>generate</strong>
                  pictograms directly from language.</td>
              </tr>
              <tr>
                <td><em>What to design</em></td>
                <td>A <strong>transformer-based LLM</strong> that converts natural language into <strong>SVG</strong>
                  pictograms—using a visual grammar linking verbs, objects, and contexts.</td>
              </tr>
              <tr>
                <td><em>Why it matters</em></td>
                <td>Builds a bridge between text and image, expanding communication for people with complex
                  communication needs.</td>
              </tr>
            </tbody>
            </table>
            <object type="image/svg+xml" class="thumb" data="/svg/level-1.svg"></object>
          </div>
          
          <aside class="notes">
            <p>The transformer that turns words into images. It wrestles with the old problem of representation: what
            travels from text to picture, what is lost, what can be learned.</p>
            <p>We ground this in Wittgenstein (meaning as use), Austin/Searle (speech acts), and the Natural Semantic
            Metalanguage (shared “atoms” of meaning). PictoNet maps those semantic primes to visual primitives.</p>
          </aside>
          
        </section>

        <!-- Sub-slide 3.2 -->
        <section>
          <div class="col-2">
            <table class="essence-mediafranca level level-pictoforge" border="0" cellpadding="8" cellspacing="0">
          <thead>
            <tr>
              <th><span>Focus</span></th>
              <th><strong>PictoForge</strong><br><em>Designing with AI</em></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><em>The challenge</em></td>
              <td>Treat AI as a <strong>design material</strong>—directly manipulable, refineable, and understandable
                through use.</td>
            </tr>
            <tr>
              <td><em>Where the gap is</em></td>
              <td>Most generative tools hide inner logic; users cannot see how outputs are built or teach the system how
                to improve.</td>
            </tr>
            <tr>
              <td><em>What to design</em></td>
              <td>An <strong>interactive interface</strong> where users adjust pictograms and each correction becomes
                structured feedback for retraining (direct manipulation & RLHF).</td>
            </tr>
            <tr>
              <td><em>Why it matters</em></td>
              <td>Turns machine learning into a visible, human process—understanding grows through collaboration, not
                opacity.</td>
            </tr>
          </tbody>
          </table>
          <object type="image/svg+xml" class="thumb" data="/svg/level-2.svg"></object>
          </div>
    
          <aside class="notes">
            <p>The workshop where people and model co-edit pictograms. Corrections, audits, and local vocabularies are
            logged as structured data. Each human nudge becomes a learning signal.</p>
          <p>This is the <strong>corrective/generative loop</strong>: propose → edit → learn. Over time the system
            internalises clarity, cultural fit, and adult dignity.</p>
          </aside>
          
        </section>

        <!-- Sub-slide 3.3 -->
        <section>
          <div class="col-2">
            <table class="essence-mediafranca level level-mediafranca" border="0" cellpadding="8" cellspacing="0">
            <thead>
              <tr>
                <th><span>Focus</span></th>
                <th><strong>MediaFranca</strong><br><em>Language as a Commons</em></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><em>The challenge</em></td>
                <td>Ensure the <strong>fair use of data</strong> for AI training—respecting authorship, privacy,
                  cultural identity, and model portability.</td>
              </tr>
              <tr>
                <td><em>Where the gap is</em></td>
                <td>Centralised infrastructures concentrate data and power; communities lose control over voices,
                  languages, and creative work.</td>
              </tr>
              <tr>
                <td><em>What to design</em></td>
                <td>A <strong>federated, open-source platform</strong> where local instances train their models and
                  share improvements responsibly (cultural sovereignty & ethical governance).</td>
              </tr>
              <tr>
                <td><em>Why it matters</em></td>
                <td>Redefines AI as a collective resource rather than a corporate asset—communities own, adapt, and
                  evolve their languages.</td>
              </tr>
            </tbody>
          </table>
          <object type="image/svg+xml" class="thumb" data="/svg/level-3.svg"></object>
          </div>
          
          <aside class="notes">
            <p>A federated, open implementation — a <em>core communication engine</em>. Communities run local instances,
            keep their data, share model improvements. Culture stays sovereign; learning flows.</p>
          <p>Result: dialects of a shared pictographic grammar, maintained as a civic commons.</p>
          </aside>
          
        </section>
      </section>

      <!-- Slide 4 -->
      <section>
        <h3>The problem of language</h3>
        <p>Early logic dreamed of purity — words mirroring facts. But daily speech is relational and embodied; meaning
          lives in context, gesture, timing.</p>
        <p>In AAC, a single selection can carry a whole emotion. So we move from truth-conditions to
          <strong>use-conditions</strong> — from logic to pragmatics.
        </p>
        <aside class="notes">
          Short contrast: “map vs territory”. Bring audience with you.
        </aside>
      </section>

      <section>
        <h3>From logic to pragmatism</h3>
        <p>Wittgenstein: meaning is use. Austin: to speak is to act. Pictograms perform: request, comment, reject,
          connect.</p>
        <p>Experts tell us: a good pictogram isn’t the one that looks right; it’s the one that <em>works</em> in a
          social moment. Success is communicative, not aesthetic.</p>
        <aside class="notes">
          Tie theory to field evidence. Keep verbs active: request/comment/reject/connect.
        </aside>
      </section>

      <section>
        <h3>Isotype & the democracy of information</h3>
        <p>Neurath and Arntz turned statistics into pictures so everyone could read society. Design as translation;
          information as a public good.</p>
        <p>MediaFranca carries that ethic forward: not simplification, but shared structure and open participation.</p>
        <aside class="notes">
          One image of classic Isotype charts works well here.
        </aside>
      </section>

      <section>
        <h3>A short genealogy of AAC pictography</h3>

        <section>
          <h4>Blissymbolics</h4>
          <p>Logical, compositional, universal in aim — conceptually rich, but hard to learn and far from lived context.
          </p>
        </section>

        <section>
          <h4>PECS</h4>
          <p>Language as exchange. In practice, cards become tokens of agency — children hide “cake” to trade later. The
            first spark of intentional communication.</p>
        </section>

        <section>
          <h4>Core Boards</h4>
          <p>High-frequency words in pragmatic layouts — pattern over script, inviting improvisation and conversation.
          </p>
        </section>

        <section>
          <h4>Towards PictoNet</h4>
          <p>Rigour (Bliss) + intent (PECS) + adaptability (Core) — now with a model that learns back from users.</p>
        </section>

        <aside class="notes">
          Keep this moving; one crisp story per sub-slide.
        </aside>
      </section>

      <section>
        <h3>Cognition & culture</h3>
        <p>Dual Coding: minds weave verbal and visual threads together. Yet culture colours both — there’s no neutral
          picture.</p>
        <p>Practitioners describe the ladder of abstraction: real object → photo → pictogram. For some, stick figures
          fail; faces need eyes to feel human.</p>
        <p>Design response: maximise iconic transparency for concrete things, and provide a consistent visual grammar
          for abstract ideas.</p>
        <aside class="notes">
          Name the “universal–local paradox”: shareable yet belonging.
        </aside>
      </section>

      <section>
        <h3>NSM — tiny atoms of meaning</h3>
        <p>Semantic primes like <em>want, do, good, someone, because</em> appear in every language. PictoNet maps these
          to visual primitives — strokes, shapes, gestures — then composes richer “molecules”.</p>
        <p>Shared grammar, local accents. Universality in structure, specificity in expression.</p>
        <aside class="notes">
          If you show a table, keep it minimal — 8–12 primes is plenty.
        </aside>
      </section>

      <section>
        <h3>Pragmatics in action</h3>
        <p>Frames shape meaning: the same icon behaves differently in different scenes. “Cake” can signal hunger,
          celebration, or reward.</p>
        <p>By recognising frames and speech-acts, the system generates pictograms that not only represent — they
          <em>perform</em> the intended act.
        </p>
        <aside class="notes">
          Offer a quick pair: raised hand = greeting vs refusal, depending on frame.
        </aside>
      </section>

      <section>
        <h3>Training the model</h3>
        <p>Between text and image lies a high-dimensional landscape. Training teaches the model to find regions where
          clarity lives: enough detail to mean, not so much that it overwhelms.</p>
        <p>The loop: text → SVG → human edit → learn. Each correction sharpens semantic precision and reduces cognitive
          load. Never perfect; always improving — like human learning.</p>
        <aside class="notes">
          Keep maths implicit. Anchor in outcomes: clarity, simplicity, recognisability.
        </aside>
      </section>

      <section>
        <h3>PictoForge — the human–AI workshop</h3>
        <p>Practitioners co-edit outputs; every drag, rename, simplification, or swap is captured as structured
          feedback. Through RLHF, the model learns from human judgement, not just from data distributions.</p>
        <p>Audits ensure quality; local vocabularies ensure cultural fit. Style becomes semantics through practice.</p>
        <aside class="notes">
          If you demo, show a side-by-side: model proposal → edited result → logged diff.
        </aside>
      </section>

      <section>
        <h3>MediaFranca — federated & open</h3>
        <p>No single centre. Communities run local instances; data stays local; model updates travel. Chile learns from
          Aotearoa; Aotearoa from the Pacific — without losing their voice.</p>
        <p>From app to <em>commons</em>: a civic engine for shared understanding.</p>
        <aside class="notes">
          Phrase to land: “Models talk, not data.” Invite questions on privacy & sovereignty.
        </aside>
      </section>

      <section>
        <h3>Rationale & significance</h3>
        <p>From Isotype’s democratic clarity to today’s open AI, this project treats communication as public
          infrastructure. It bridges philosophy, linguistics, cognition, and engineering in one practical artefact.</p>
        <p>PictoNet (engine), PictoForge (practice), MediaFranca (commons): three names for one move — from designing
          symbols to designing relationships.</p>
        <aside class="notes">
          Reconnect ethics: adult dignity, cultural adaptability, accessibility as agency.
        </aside>
      </section>

      <section>
        <h3>Closing invitation</h3>
        <p>Each layer is an open question, held in open repositories. We welcome contributions to code, datasets, and
          design — from professionals, families, and communities.</p>
        <p>Help refine the pictograms. Teach the model. Grow local vocabularies. Shape a language that belongs to
          everyone.</p>
        <p><strong>MediaFranca isn’t finished.</strong> It’s a place to think together, draw together, and keep language
          alive.</p>
        <aside class="notes">
          End on community and agency. Invite students to follow, fork, and contribute.
        </aside>
      </section>
    </div>
  </div>
  <!-- tu entry de Vite (ruta relativa para que funcione en /prs/) -->
  <script type="module" src="./main.js"></script>

</body>

</html>