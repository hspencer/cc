<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MediaFranca</title>

  <!-- en <head> o justo antes de </body>, pero SIEMPRE antes de main.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/2.0.4/p5.min.js" defer></script>
  <script src="./p5/sketches.js" defer></script>
  <script type="module" crossorigin src="/cc/assets/index-DMCoikJi.js"></script>
  <link rel="stylesheet" crossorigin href="/cc/assets/index-lUhmRPFN.css">
</head>

<body>
  <div class="reveal">
    <div class="slides">

      <!-- Slide 1: Title  -->
      <section data-background-color="#FFF" data-transition="concave" id="title-slide">
        <img src="svg/title-illustration.svg" alt="MediaFranca Illustration: a thinking bubble of pictograms" class="title-illustration">
        <p class="tagline">MediaFranca</p>
        <h1 class="title">A Practice-Oriented Investigation into a Generative Pictographic System for Cognitive Accessibility</h1>
        <div class="credentials">
          <div class="author">
            <strong>Herbert Spencer González</strong><br>
            PhD Student in Design<br>
            <span>See this presentation at <a href="https://herbertspencer.net/cc">herbertspencer.net/cc</a></span>
          </div>


          <div class="tutors">
            <dt>Supervisors</dt>
            <dl>Dr. Marcos Steagall</dl>
            <dl>Dr. Ivana Nakarada-Kordic</dl>
            <dt>Advisor</dt>
            <dl>Dr. Welby Ings</dl>
          </div>

          <div class="flex-container"><img class="logo" src="images/AUT.jpg" alt="Auckland University of Technology">
          </div>
          <aside class="notes">
            I am Herbert Spencer González, a PhD student in Design at AUT. My research is about creating a generative pictographic system that can expand the ways we communicate, especially for those whose voices are often excluded. This work is supervised by Marcos Steagall and Ivana Nakarada-Kordic, with Welby Ings as advisor. The aim is not only technical but deeply ethical: how design can open communication as a right, not as a privilege.
          </aside>
        </div>


      </section>

      <!-- Slide 2: About Me  -->
      <section data-background-color="#FFF">

        <!-- Aromos  -->
        <section data-background-image="images/aromos.jpg">
          <aside class="notes">This is my home in Chile, near the Aconcagua river and Cerro la Campana. I begin here because design is always situated; it begins from place, from the ground where we stand and from the waters that sustain us.</aside>
        </section>

        <!-- Familia  -->
        <section data-background-image="images/family.jpg">
          <h2 class="photo-overlay"><span>My family</span></h2>
          <aside class="notes">This is my family. I come from a lineage that is diverse and mixed, and my grandmother, who just turned one hundred, embodies endurance and care. Family is a reminder that communication is never abstract; it is always relational.</aside>
        </section>

        <!-- Profesor  -->
        <section data-background-image="images/travesia.jpg" >
          <h2 class="photo-overlay"><span>Travesía 2018</span><br><span>Corredor Litoral</span><br><span class="light">First Year Design Studio</span><span>+</span><br><span class="light">Third year Light and Color Design Studio</span></h2>
          
          <aside class="notes">Here you see my students during a design "Travesía" in the north of Chile. We travelled where the Pacific meets the Atacama desert and built, at full scale, a timber “coastal corridor”. For me, teaching design has always meant expanding perception, opening encounters, and making space for others to participate.</aside>
        </section>

        <!-- Investigación Inclusiva  -->
        <section data-background-image="images/inv-inclusiva.jpg" >
          <h2 class="photo-overlay"><span class="light">Initial round of</span><br><span>Interdisciplinary Inclusive Research</span><br><span>2017</span></h2>
          <aside class="notes">
            In 2017, I was invited to collaborate with researchers in special education. That invitation changed my trajectory. I began working on disability-related projects, with a focus on self-determination and quality of life. Design here became less about objects and more about enabling lives.
          </aside></aside>
        </section>

        <!-- Núcleo de Investigación en Accesibilidad e Inclusión PUCV  -->
        <section data-background-image="images/nucleo.jpg" >
          <h2 class="photo-overlay"><span class="light">Research Centre for</span><br><span>Accessibility and Inclusion</span><br><span class="light">Pontifical Catholic University of Valparaíso</span></h2>
          <aside class="notes">
            Over time, the team grew into an interdisciplinary centre, with people from linguistics, engineering, sociology, law and more. Together we tried to build knowledge across differences, following the idea that inclusive research requires inclusive teams.
          </aside></aside>
        </section>

        <!-- Inclusive Research and cognitive accessibility  -->
        <section data-background-image="images/pictos-metro.jpg">
          <h2 class="photo-overlay"><span>The PICTOS proyect</span><br><span class="light">Visual and Procedural Supports for</span><br><span>Cognitive Accessibility of Public Services</span></h2>
          <aside class="notes">
            With the PICTOS project we listened directly to adults with intellectual disabilities. They told us that their greatest challenge was not lack of services but lack of autonomy when facing inaccessible systems. Health, transport, bureaucracy, even leisure became barriers. That insight transformed our agenda.
          </aside>
        </section>

        <!-- PICTOS Service  -->
        <section class="w">
          <img class="fit" src="images/pictos-service.png" alt="PICTOS Sevice Arqchitecture">
          <aside class="notes">
            So we designed supports: a grammar for pictograms, a system for plain language, and a publishing architecture to adapt these supports across services. The aim was not to simplify life but to return dignity, to enable people to complete everyday transactions without dependence... and get the value that these services offer.
          </aside>
        </section>

        <!-- Exponential tech growth  -->
        <section class="w">
          <div class="p5-host" data-sketch="chronology"></div>
          <aside class="notes">As designers we are witnesses to exponential technological growth. From digital tools to generative AI, the landscape shifts beneath our feet. We need not retreat from this but confront it as an opportunity. That is why I am here, embarking on a PhD, to think of AI not as threat but as design material.</aside>
        </section>

        <!-- Distance  -->
        <section 
        data-background-color="#0094f5" 
        data-background-image="svg/map.svg" >
        <aside class="notes">And so I find myself here in Aotearoa, across the Pacific, starting a new chapter, carrying with me both my home and the responsibility to make this work matter.</aside>
        </section>

      </section>

      <!-- Slide 3: Research question -->
      <section>
        <section>
          <p class="huge question">
            How can a generative pictographic system <span>be designed</span> to support
            communication for people with complex communication needs?
          </p>
          <aside class="notes">The core question I bring here is: how can a generative pictographic system be designed to support communication for people with complex communication needs? To approach this, I am not working alone in my studio. I am conducting interviews with two groups of professionals. On the one hand, those who teach, educate, and promote the use of AAC tools in schools, clinics, and daily practices. On the other hand, designers who have dedicated their craft to pictograms with great concern for accessibility, comprehension, synthesis, and elegance. Their voices frame the territory of this research.</aside>
        </section>

        <!-- Coreboard -->
        <section data-background-color="#FFF" data-transition="fade">
          <img src="images/core-board.png" alt="Core Board as used in New Zealand" class="r-stretch">
          <aside class="notes">
            This is a Core Board used in New Zealand as an alternative and augmentative tool for communication or AAC. It contains a set of high-frequency words that can be combined to create sentences. It is used by people with complex communication needs,
            often in conjunction with speech-generating devices.
            Therapists remind me that while these boards enable expression, they also expose their limits. They are static, styles vary, and abstraction levels are inconsistent. AAC professionals stress the frustration that arises when pictograms are not visually coherent or when key vocabulary is missing. They say, “a symbol is only useful if it can be quickly recognised and recombined.” This is a clinical truth, but it is also a design challenge.
          </aside>
        </section>


        <section data-background-iframe="https://pictogramas.pictos.cl" data-background-interactive
          data-background-opacity="1" class="zoom125">
          <aside class="notes">
            PICTOS pictogramas are constructed as the sum of 3 layers. The complete support is the pictogram that represents the situation, the instruction in writen and spoken word and an icon that reinforces the action.
            From the designers I interviewed, a different concern emerges. They insist that a good pictogram is not just guessable; it must hold clarity without losing dignity. They care about stroke weight, proportion, and balance, because these small decisions affect legibility and trust. One told me, “elegance is not decoration; it is what keeps the image memorable.” That resonates with the PICTOS approach, where we built supports in layers to reduce cognitive load.
          </aside>
        </section>

        <!-- the space between words and images -->
        <section 
        data-background-color="#FFF" 
        data-background-image="svg/word-visual-space.svg" >
        <aside class="notes">Both groups converge here: they say cognitive accessibility depends on coherence between words and images. Therapists describe the fatigue of their students when facing mismatched systems. Designers describe the failure of icons when they drift into opacity or cliché. Their insights confirm the hypothesis: the corridor between word and image must remain tight, structured, and open to correction.
        The PICTOS approach consists of a highly coherent support between the word space and the image. The direct correspondence between instruction and pictogram promotes comprehension and reduces cognitive overload. This is the fundamental idea on which my research is based.</aside>
        </section>

        <!-- Lectogram -->
        <section data-background-iframe="https://herbertspencer.net/lectogram/" frameborder="0" class="zoom133">
          <aside class="notes">
            In the Lectogram project, I began composing pictograms as visual phrases. Here again the interviews proved decisive. AAC professionals explained that in real practice, users rarely rely on single symbols; they combine them. Designers explained that composition requires rules of rhythm, spacing, and hierarchy. This is why I now define a pictogram not as a single glyph but as a structured phrase, a choreography of iconic elements that together convey meaning with clarity and care.
          </aside>
        </section>

      </section>

      <!-- SVG as Code ↔ SVG as Image - section of sections -->
      <section>
        <section id="svg-code-sync" data-transition="fade">
          <h2>Text as Image</h2>

          <div style="display:grid;grid-template-columns:50% 50%;height:500px;gap:1rem;">

            <!-- IZQUIERDA: código -->
            <div style="overflow:auto;font-size:.55em;line-height:1.2;">
              <pre style="font-size:85%;white-space:pre;margin-left: 2em;">
                <code id="svg-source" class="language-xml" data-line-numbers></code>
              </pre>
            </div>

            <!-- DERECHA: contenedor para SVG inline (sustituye el <object> existente) -->
            <div id="svg-stage" style="display:flex;align-items:center;justify-content:center;background:#9c9b9b;border-radius:1ex;">
            </div>

            <!-- script del round trip hightlight -->

          </div>

          <aside class="notes">
            Here is the thing: SVG is both an image format and a text-based code format. This means that you can edit it as text, and the image will change accordingly. Note that the code holds verosimilitude to the semantic structure of the image. This opens a more granular way to edit and adapt pictograms, and for the system to learn from those edits. When I speak of SVG as both image and code, I return to what designers told me: transparency is essential. One designer remarked, “a pictogram must not be an ornament you cannot touch; it should be something you can open, adjust, and rebuild.” On the other side, AAC professionals underlined the importance of editability: when a pictogram does not match a child’s context, they need to adapt it quickly. SVG offers precisely this: structure that can be read, modified, and taught to machines, but also kept open for human correction. It transforms the pictogram from a closed asset into a living artefact.

          </aside>
        </section>

      </section><!-- fin de code-as-svg -->

      <!-- PictoNet Generative Pipeline -->
      <section
        data-background-gradient="linear-gradient(173deg,rgba(2, 20, 20, 1) 1%, rgba(36, 40, 43, 1) 61%, rgba(30, 46, 29, 1) 100%)">
        <h2 style="color: orangered">PictoNet Generative Pipeline</h2>
        <p class="tagline" style="color: rgba(250, 235, 215, 0.485);">From text input to adaptive pictogram output</p>
        
        <section>
          <div
            class="p5-host" 
            data-sketch="neural-network" 
            data-node-size="10"
            data-frame-rate="8">
          </div>
          <aside class="notes">But LLMs are, by definition, black boxes. They are complicated, obfuscated, and not designed to be transparent nor cognitively accesible for humans. But, what if we can deconstruct the design process behind a pictogram, and use that as a pipeline to guide the LLM in generating pictograms? This is one line of my initial enquiry, were I'm interviewing pictogram designers. Large language models are powerful, but as AAC educators told me, they are “opaque to the very people who need them to be explainable.” They fear systems that deliver results without logic, because teaching AAC requires explaining why a symbol is what it is. Designers echoed this: they want a generative system that carries the reasoning of design, not only its output. This is why I frame the pipeline as a way to domesticate AI, forcing it to follow the design process: from meaning extraction to visual mapping, from composition to adaptation. The model is not asked to invent; it is asked to learn design.</aside>
        </section>
        

        <section data-background-color="transparent">
          <img src="/cc/svg/pipeline-1.svg" class="r-stretch" style="background-color: transparent;">
          <aside class="notes">
            From one end we input text (voice or written), which is then processed through Natural Language Processing to extract semantic features. These features are then mapped to visual elements using a predefined set of design rules and principles. Finally, the visual elements are composed into a coherent pictogram that effectively communicates the intended message.

            From interviews with AAC professionals, a recurring insight is that words need to be disambiguated before pictorial translation. They reminded me of the chaos when a single term like “light” becomes a bulb in one context, the sun in another, and “not heavy” in a third. Semantic clarity is non-negotiable. Designers add a parallel concern: each concept requires multiple valid views — minimal, detailed, metaphorical — that can be chosen depending on need. This is the foundation of the first pipeline stage: meaning must be stabilised, then opened into variants without collapsing its core.
          </aside>
        </section>

        <section>
          <img src="/cc/svg/pipeline-2.svg" class="r-stretch">
          <aside class="notes">
            The system is designed to be adaptive, allowing for customization based on user preferences and context. This includes adjusting the level of detail, color schemes, and cultural considerations to ensure the pictograms are both effective and inclusive.

            In therapy rooms, professionals insist on predictability: “a system must keep visual consistency, otherwise learning is wasted.” This means composition rules are not aesthetic luxuries but learning scaffolds. Designers, for their part, speak of elegance as memorability: spacing, rhythm, and stroke thickness are what allow an icon to be recalled under pressure. Together these voices argue for adaptation, not randomisation. Cultural markers, colour palettes, abstraction levels — all these can shift, but only within rules that keep memory and trust intact.
          </aside>
        </section>

        <section>
          <img src="/cc/svg/pipeline-3.svg" class="r-stretch">
        </section>
        <aside class="notes">
          The goal of this generative pipeline is to create a flexible and scalable system that can produce high-quality pictograms on demand, enhancing communication for individuals with complex communication needs.

          When I asked AAC practitioners about correction, they said the worst systems are those that never learn. They want to annotate, to flag when a symbol confuses, and to see the system evolve. Designers add caution: they fear “contamination” of the visual language if every correction is accepted without filter. This tension shaped my proposal of VCSCI, the Visual Communicability and Semantic Correspondence Index: a small set of indicators that can measure clarity, communicability, and cognitive load. In this way, feedback is not arbitrary; it is guided, weighted, and made traceable. The system learns, but it learns responsibly.
        </aside>
      </section>

      <!-- NSM Semantic Primes -->
      <section data-background-gradient="linear-gradient(173deg,rgba(2, 20, 20, 1) 1%, rgba(36, 40, 43, 1) 61%, rgba(30, 46, 29, 1) 100%)"
      data-transition="slide">
        <h2 class="title">65 semantic primes</h2> 
        
        <main class="nsm" >
            <!-- 1. Substantives -->
            <div>
                <h2 class="block-name">Substantives</h2>
                <ul class="nsm-block">
                    <li class="d1">I</li>
                    <li class="d2">YOU</li>
                    <li class="d1">SOMEONE / PERSON</li>
                    <li class="d1">SOMETHING / THING</li>
                    <li class="d1">BODY</li>
                    <li class="d2">PEOPLE</li>
                </ul>
              </div>

            <!-- 2. Determiners & Quantifiers -->
            <div>
                <h2 class="block-name">Determiners & Quantifiers</h2>
                <ul class="nsm-block">
                    <li class="d1">THIS</li>
                    <li class="d2">THE SAME</li>
                    <li class="d3">OTHER / ELSE</li>
                    <li class="d1">ONE</li>
                    <li class="d1">TWO</li>
                    <li class="d2">MANY / MUCH</li>
                    <li class="d3">SOME / A FEW</li>
                    <li class="d4">ALL</li>
                    <li class="d5">THERE IS / EXISTS</li>
                    <li class="d4">HAVE (PARTS)</li>
                </ul>
              </div>

            <!-- 3. Actions & Events -->
            <div>
                <h2 class="block-name">Actions & Events</h2>
                <ul class="nsm-block">
                    <li class="d1">DO</li>
                    <li class="d4">HAPPEN</li>
                    <li class="d1">MOVE</li>
                    <li class="d1">TOUCH</li>
                </ul>
              </div>

            <!-- 4. Mental Predicates -->
            <div>
                <h2 class="block-name">Mental Predicates</h2>
                <ul class="nsm-block">
                    <li class="d3">THINK</li>
                    <li class="d4">KNOW</li>
                    <li class="d5">WANT</li>
                    <li class="d5">FEEL</li>
                    <li class="d4">SEE</li>
                    <li class="d4">HEAR</li>
                </ul>
              </div>

            <!-- 5. Speech -->
            <div>
                <h2 class="block-name">Speech</h2>
                <ul class="nsm-block">
                    <li class="d2">SAY</li>
                    <li class="d3">WORD</li>
                    <li class="d5">TRUE</li>
                </ul>
              </div>

            <!-- 6. Time & Place -->
            <div>
                <h2 class="block-name">Time & Place</h2>
                <ul class="nsm-block">
                    <li class="d5">WHEN / TIME</li>
                    <li class="d3">NOW</li>
                    <li class="d3">BEFORE</li>
                    <li class="d3">AFTER</li>
                    <li class="d5">A LONG TIME</li>
                    <li class="d5">A SHORT TIME</li>
                    <li class="d4">WHERE / PLACE</li>
                    <li class="d2">HERE</li>
                    <li class="d1">ABOVE</li>
                    <li class="d1">BELOW</li>
                    <li class="d1">FAR</li>
                    <li class="d1">NEAR</li>
                    <li class="d1">SIDE</li>
                    <li class="d1">INSIDE</li>
                </ul>
              </div>

            <!-- 7. Logical Concepts -->
            <div>
                <h2 class="block-name">Logical Concepts</h2>
                <ul class="nsm-block">
                    <li class="d2">NOT / NO</li>
                    <li class="d4">MAYBE</li>
                    <li class="d4">CAN</li>
                    <li class="d5">BECAUSE</li>
                    <li class="d5">IF</li>
                </ul>
              </div>

            <!-- 8. Evaluators & Descriptors -->
            <div>
                <h2 class="block-name">Evaluators & Descriptors</h2>
                <ul class="nsm-block">
                    <li class="d4">GOOD</li>
                    <li class="d4">BAD</li>
                    <li class="d2">BIG</li>
                    <li class="d2">SMALL</li>
                    <li class="d3">VERY</li>
                    <li class="d3">LIKE / AS</li>
                </ul>
              </div>
        </main>
        <p class="light"><strong>Wierzbicka, A.</strong> (1996, revised 2017). Natural Semantic Metalanguage<br>Revised set of 65 semantic primes. London: Oxford University Press.</p>
        <aside class="notes">Natural Semantic Metalanguage emerged as a tool that both groups intuitively endorsed. Educators appreciated it because it limits the vocabulary to concepts that can be taught without circular explanations: WANT, KNOW, FEEL, DO. Designers appreciated it because primes force clarity at the visual level, reducing the temptation to lean on cultural clichés. One AAC specialist told me, “when we teach ‘WANT’, it must be unmistakable, otherwise the whole system collapses.” The primes provide that anchor, allowing a pictographic grammar to be built with universality at its base and cultural richness layered on top.</aside>
      </section>

      <!-- PictoForge -->
      <section
        data-background-image="images/pictoforge.png"
        class="pf-slide"
      >
        <!-- Overlay que aparece con el primer clic -->
        <img
          src="images/pictoforge-edit.png"
          alt="Segment Editor"
          class="fragment fade-in-then-out pf-overlay"
          style="width: 400px; margin: 0 30% 0 30%"
          data-fragment-index="1"
        >
        <aside class="notes">
          This is a proposal for PictoForge, the interface to operate the PictoNet model. Educators can adjust pictograms to individual contexts — a classroom, a family, a cultural habit — without losing overall coherence and designers see a way to capture corrections not as ad hoc edits but as structured contributions: a change in stroke, a swap of metaphor, a clarified gesture. They told me, “the system must record not only the outcome, but the reasoning behind each correction.” PictoForge is meant to do exactly that (RLHF), keeping edits transparent and reusable so that human expertise flows back into the model.
        </aside>
      </section>

      <!-- 3 layered project -->
      <section data-background-gradient="linear-gradient(176deg,rgba(232, 225, 247, 1) 0%, rgba(227, 247, 250, 1) 100%)" >
        <object data="svg/3-research-layers.svg" class="r-stretch"></object>
        <aside class="notes">So, this research is structured across three layers. At the first level, the pictogram itself: what makes it good, clear, and memorable — as AAC professionals say, “learnable and consistent,” and as designers add, “synthetic and elegant.” At the second level, the interaction: how humans and generative systems co-compose, how reversibility and transparency are guaranteed, how edits remain explainable. At the third level, the infrastructure: how corrections become collective, how cultural adaptations are federated, and how governance prevents decay into noise. This threefold structure is not only my method; it is also my ethical stance.</aside>
      </section>


      <!-- Thank you, contact & information -->
      <section data-background-gradient="linear-gradient(176deg,rgba(232, 225, 247, 1) 0%, rgba(227, 247, 250, 1) 100%)" data-transition="fade" class="last">
        <h2>Thank you<br>...any questions or comments?</h2>

        <p>Please continue the conversation: <a href="mailto:herbert.spencer@aut.ac.nz">herbert.spencer@aut.ac.nz</a></p>

        <h4>Open Source</h4>
        <ul>
          <li><a href="https://github.com/mediafranca/pictonet">PictoNet</a></li>
          <li><a href="https://github.com/mediafranca/pictoforge">PictoForge</a></li>
          <li><a href="https://github.com/mediafranca/VCSCI">Visual Communicability and Semantic Correspondence Index</a></li>
          <li><a href="https://github.com/mediafranca/manifesto">Design Manifesto</a></li>
        </ul>

        <p class="small">This presentation lives at <a href="https://herbertspencer.net/prs">herbertspencer.net/prs</a>.<br>
          Source code at <a href="https://github.com/hspencer/prs">github.com/hspencer/prs</a>
        </p>

      </section>

    </div>
  </div>
  <!-- tu entry de Vite (ruta relativa para que funcione en /prs/) -->

</body>
</html>